{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Basics"
      ],
      "metadata": {
        "id": "YYYj9cnxZaT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n"
      ],
      "metadata": {
        "id": "hKHHgjrucCeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd9e244c-5635-428b-a040-e811ff0e9a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "HmDwwajaGmlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36044b74-65b4-4c2e-c5df-6f819fa4c955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow"
      ],
      "metadata": {
        "id": "8dbxLQaOjR48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H_tcesJb8iV"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/gdrive/MyDrive/FYP_Project/')"
      ],
      "metadata": {
        "id": "m6Mb5uyuM0sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "zFEtkU9IVCbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_glovepath = '/content/gdrive/MyDrive/FYP_Project/glove.6B.300d.txt'"
      ],
      "metadata": {
        "id": "h5DjIuTElUwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import json\n",
        "from operator import itemgetter\n",
        "\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from collections import namedtuple\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "class Embedder(object):\n",
        "    \"\"\" Generic embedding interface.\n",
        "\n",
        "    Required: attributes g and N \"\"\"\n",
        "\n",
        "    def map_tokens(self, tokens, ndim=2):\n",
        "        \"\"\" for the given list of tokens, return a list of GloVe embeddings,\n",
        "        or a single plain bag-of-words average embedding if ndim=1.\n",
        "\n",
        "        Unseen words (that's actually *very* rare) are mapped to 0-vectors. \"\"\"\n",
        "        gtokens = [self.g[t] for t in tokens if t in self.g]\n",
        "        if not gtokens:\n",
        "            return np.zeros((1, self.N)) if ndim == 2 else np.zeros(self.N)\n",
        "        gtokens = np.array(gtokens)\n",
        "        if ndim == 2:\n",
        "            return gtokens\n",
        "        else:\n",
        "            return gtokens.mean(axis=0)\n",
        "\n",
        "    def map_set(self, ss, ndim=2):\n",
        "        \"\"\" apply map_tokens on a whole set of sentences \"\"\"\n",
        "        return [self.map_tokens(s, ndim=ndim) for s in ss]\n",
        "\n",
        "    def pad_set(self, ss, spad, N=None):\n",
        "        \"\"\" Given a set of sentences transformed to per-word embeddings\n",
        "        (using glove.map_set()), convert them to a 3D matrix with fixed\n",
        "        sentence sizes - padded or trimmed to spad embeddings per sentence.\n",
        "\n",
        "        Output is a tensor of shape (len(ss), spad, N).\n",
        "\n",
        "        To determine spad, use something like\n",
        "            np.sort([np.shape(s) for s in s0], axis=0)[-1000]\n",
        "        so that typically everything fits, but you don't go to absurd lengths\n",
        "        to accomodate outliers.\n",
        "        \"\"\"\n",
        "        ss2 = []\n",
        "        if N is None:\n",
        "            N = self.N\n",
        "        for s in ss:\n",
        "            if spad > s.shape[0]:\n",
        "                if s.ndim == 2:\n",
        "                    s = np.vstack((s, np.zeros((spad - s.shape[0], N))))\n",
        "                else:  # pad non-embeddings (e.g. toklabels) too\n",
        "                    s = np.hstack((s, np.zeros(spad - s.shape[0])))\n",
        "            elif spad < s.shape[0]:\n",
        "                s = s[:spad]\n",
        "            ss2.append(s)\n",
        "        return np.array(ss2)\n",
        "\n",
        "class GloVe(Embedder):\n",
        "    \"\"\" A GloVe dictionary and the associated N-dimensional vector space \"\"\"\n",
        "    def __init__(self, N=300, glovepath=drive_glovepath):\n",
        "        \"\"\" Load GloVe dictionary from the standard distributed text file.\n",
        "\n",
        "        Glovepath should contain %d, which is substituted for the embedding\n",
        "        dimension N. \"\"\"\n",
        "        self.N = N\n",
        "        self.g = dict()\n",
        "        self.glovepath = glovepath\n",
        "\n",
        "        # with open('glove.6B.300d.txt') as f:\n",
        "        with open(self.glovepath) as f:\n",
        "            for line in f:\n",
        "                l = line.split()\n",
        "                word = l[0]\n",
        "                self.g[word] = np.array(l[1:]).astype(float)\n",
        "\n",
        "\n",
        "def hash_params(pardict):\n",
        "    ps = json.dumps(dict([(k, str(v)) for k, v in pardict.items()]), sort_keys=True)\n",
        "    h = hash(ps)\n",
        "    return ps, h\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "NLP preprocessing tools for sentences.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "flagsdim = 4\n",
        "\n",
        "def sentence_flags(s0, s1, spad):\n",
        "    \"\"\" For sentence lists s0, s1, generate numpy tensor\n",
        "    (#sents, spad, flagsdim) that contains a sparse indicator vector of\n",
        "    various token properties.  It is meant to be concatenated to the token\n",
        "    embedding. \"\"\"\n",
        "\n",
        "    def gen_iflags(s, spad):\n",
        "        iflags = []\n",
        "        for i in range(len(s)):\n",
        "            iiflags = [[False, False] for j in range(spad)]\n",
        "            for j, t in enumerate(s[i]):\n",
        "                if j >= spad:\n",
        "                    break\n",
        "                number = False\n",
        "                capital = False\n",
        "                if re.match('^[0-9\\W]*[0-9]+[0-9\\W]*$', t):\n",
        "                    number = True\n",
        "                if j > 0 and re.match('^[A-Z]', t):\n",
        "                    capital = True\n",
        "                iiflags[j] = [number, capital]\n",
        "            iflags.append(iiflags)\n",
        "        return iflags\n",
        "\n",
        "    def gen_mflags(s0, s1, spad):\n",
        "        \"\"\" generate flags for s0 that represent overlaps with s1 \"\"\"\n",
        "        mflags = []\n",
        "        for i in range(len(s0)):\n",
        "            mmflags = [[False, False] for j in range(spad)]\n",
        "            for j in range(min(spad, len(s0[i]))):\n",
        "                unigram = False\n",
        "                bigram = False\n",
        "                for k in range(len(s1[i])):\n",
        "                    if s0[i][j].lower() != s1[i][k].lower():\n",
        "                        continue\n",
        "                    # do not generate trivial overlap flags, but accept them as part of bigrams                    \n",
        "                    if s0[i][j].lower() not in stop and not re.match('^\\W+$', s0[i][j]):\n",
        "                        unigram = True\n",
        "                    try:\n",
        "                        if s0[i][j+1].lower() == s1[i][k+1].lower():\n",
        "                            bigram = True\n",
        "                    except IndexError:\n",
        "                        pass\n",
        "                mmflags[j] = [unigram, bigram]\n",
        "            mflags.append(mmflags)\n",
        "        return mflags\n",
        "\n",
        "    # individual flags (for understanding)\n",
        "    iflags0 = gen_iflags(s0, spad)\n",
        "    iflags1 = gen_iflags(s1, spad)\n",
        "\n",
        "    # s1-s0 match flags (for attention)\n",
        "    mflags0 = gen_mflags(s0, s1, spad)\n",
        "    mflags1 = gen_mflags(s1, s0, spad)\n",
        "\n",
        "    return [np.dstack((iflags0, mflags0)),\n",
        "            np.dstack((iflags1, mflags1))]\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Vocabulary that indexes words, can handle OOV words and integrates word\n",
        "embeddings.\n",
        "\"\"\"\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\" word-to-index mapping, token sequence mapping tools and\n",
        "    embedding matrix construction tools \"\"\"\n",
        "    def __init__(self, sentences, count_thres=1):\n",
        "        \"\"\" build a vocabulary from given list of sentences, but including\n",
        "        only words occuring at least #count_thres times \"\"\"\n",
        "\n",
        "        # Counter() is superslow :(\n",
        "        vocabset = defaultdict(int)\n",
        "        for s in sentences:\n",
        "            for t in s:\n",
        "                vocabset[t] += 1\n",
        "\n",
        "        vocab = sorted(list(map(itemgetter(0),\n",
        "                                filter(lambda k: itemgetter(1)(k) >= count_thres,\n",
        "                                       vocabset.items() ) )))\n",
        "        self.word_idx = dict((w, i + 2) for i, w in enumerate(vocab))\n",
        "        self.word_idx['_PAD_'] = 0\n",
        "        self.word_idx['_OOV_'] = 1\n",
        "        print('Vocabulary of %d words' % (len(self.word_idx)))\n",
        "\n",
        "        self.embcache = dict()\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word_idx:\n",
        "            self.word_idx[word] = len(self.word_idx)\n",
        "\n",
        "    def vectorize(self, slist, pad=60):\n",
        "        \"\"\" build an pad-ed matrix of word indices from a list of\n",
        "        token sequences \"\"\"\n",
        "        silist = [[self.word_idx.get(t, 1) for t in s] for s in slist]\n",
        "        if pad is not None:\n",
        "            return pad_sequences(silist, maxlen=pad, truncating='post', padding='post') \n",
        "        else:\n",
        "            return silist\n",
        "\n",
        "    def embmatrix(self, emb):\n",
        "        \"\"\" generate index-based embedding matrix from embedding class emb\n",
        "        (typically GloVe); pass as weights= argument of Keras' Embedding layer \"\"\"\n",
        "        if str(emb) in self.embcache:\n",
        "            return self.embcache[str(emb)]\n",
        "        embedding_weights = np.zeros((len(self.word_idx), emb.N))\n",
        "        for word, index in self.word_idx.items():\n",
        "            try:\n",
        "                embedding_weights[index, :] = emb.g[word]\n",
        "            except KeyError:\n",
        "                if index == 0:\n",
        "                    embedding_weights[index, :] = np.zeros(emb.N)\n",
        "                else:\n",
        "                    embedding_weights[index, :] = np.random.uniform(-0.25, 0.25, emb.N)  # 0.25 is embedding SD\n",
        "        self.embcache[str(emb)] = embedding_weights\n",
        "        return embedding_weights\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.word_idx)\n",
        "\n",
        "\"\"\"\n",
        "Evaluation tools, mainly non-straightforward methods.\n",
        "\"\"\"\n",
        "\n",
        "def aggregate_s0(s0, y, ypred, k=None):\n",
        "    \"\"\"\n",
        "    Generate tuples (s0, [(y, ypred), ...]) where the list is sorted\n",
        "    by the ypred score.  This is useful for a variety of list-based\n",
        "    measures in the \"anssel\"-type tasks.\n",
        "    \"\"\"\n",
        "    ybys0 = dict()\n",
        "    for i in range(len(s0)):\n",
        "        try:\n",
        "            s0is = s0[i].tostring()\n",
        "        except AttributeError:\n",
        "            s0is = str(s0[i])\n",
        "        if s0is in ybys0:\n",
        "            ybys0[s0is].append((y[i], ypred[i]))\n",
        "        else:\n",
        "            ybys0[s0is] = [(y[i], ypred[i])]\n",
        "\n",
        "    for s, yl in ybys0.items():\n",
        "        if k is not None:\n",
        "            yl = yl[:k]\n",
        "        ys = sorted(yl, key=lambda yy: yy[1], reverse=True)\n",
        "        yield (s, ys)\n",
        "\n",
        "def sorted_output(s0, sent,  y, ypred, q_index=1, target_q=None):\n",
        "    \"\"\"\n",
        "    Generate tuples (s0, [(y, ypred), ...]) where the list is sorted\n",
        "    by the ypred score.  This is useful for a variety of list-based\n",
        "    measures in the \"anssel\"-type tasks.\n",
        "    \"\"\"\n",
        "    ybys0 = dict()\n",
        "    for i in range(len(s0)):\n",
        "        try:\n",
        "            s0is = \" \".join(s0[i])\n",
        "        except AttributeError:\n",
        "            s0is = str(s0[i])\n",
        "        if s0is in ybys0:\n",
        "            ybys0[s0is].append((y[i], ypred[i], sent[i]))\n",
        "        else:\n",
        "            ybys0[s0is] = [(y[i], ypred[i], sent[i])]\n",
        "\n",
        "    counter = 1\n",
        "    for s, yl in ybys0.items():      \n",
        "        ys = sorted(yl, key=lambda yy: yy[1], reverse=True)\n",
        "\n",
        "        if(s == target_q):\n",
        "            print(\"Question:\")\n",
        "            print(s)\n",
        "            print()\n",
        "            print(\"Candidate answers sorted by ypred score:\")\n",
        "            for each_sent in ys:\n",
        "                print(\" \".join(each_sent[2]))\n",
        "\n",
        "        if(counter == q_index):\n",
        "            print(\"Question:\")\n",
        "            print(s)\n",
        "            print()\n",
        "            print(\"Candidate answers sorted by ypred score:\")\n",
        "            for each_sent in ys:\n",
        "                print(\" \".join(each_sent[2]))\n",
        "        counter += 1\n",
        "        \n",
        "\n",
        "        \n",
        "def mrr(s0, y, ypred):\n",
        "    \"\"\"\n",
        "    Compute MRR (mean reciprocial rank) of y-predictions, by grouping\n",
        "    y-predictions for the same s0 together.  This metric is relevant\n",
        "    e.g. for the \"answer sentence selection\" task where we want to\n",
        "    identify and take top N most relevant sentences.\n",
        "    \"\"\"\n",
        "    rr = []\n",
        "    for s, ys in aggregate_s0(s0, y, ypred):\n",
        "        if np.sum([yy[0] for yy in ys]) == 0:\n",
        "            continue  # do not include s0 with no right answers in MRR\n",
        "        ysd = dict()\n",
        "        for yy in ys:\n",
        "            if yy[1][0] in ysd:\n",
        "                ysd[yy[1][0]].append(yy[0])\n",
        "            else:\n",
        "                ysd[yy[1][0]] = [yy[0]]\n",
        "        rank = 0\n",
        "        for yp in sorted(ysd.keys(), reverse=True):\n",
        "            if np.sum(ysd[yp]) > 0:\n",
        "                rankofs = 1 - np.sum(ysd[yp]) / len(ysd[yp])\n",
        "                rank += len(ysd[yp]) * rankofs\n",
        "                break\n",
        "            rank += len(ysd[yp])\n",
        "        rr.append(1 / float(1+rank))\n",
        "\n",
        "    return np.mean(rr)\n",
        "\n",
        "def mapcalc(s0, y, ypred):\n",
        "    \"\"\"\n",
        "    Compute MAP (mean average precision) of y-predictions, by grouping\n",
        "    y-predictions for the same s0 together.  This metric is relevant\n",
        "    e.g. for the \"answer sentence selection\" task where we want to\n",
        "    identify and take top N most relevant sentences.\n",
        "    \"\"\"\n",
        "\n",
        "    rr = []\n",
        "  \n",
        "    for s, ys in aggregate_s0(s0, y, ypred):\n",
        "        temp = []\n",
        "        if np.sum([yy[0] for yy in ys]) == 0:\n",
        "            continue  # do not include s0 with no right answers in MRR\n",
        "        ysd = dict()\n",
        "        for yy in ys:\n",
        "            if yy[1][0] in ysd:\n",
        "                ysd[yy[1][0]].append(yy[0])\n",
        "            else:\n",
        "                ysd[yy[1][0]] = [yy[0]]\n",
        "        rank = 0\n",
        "        counter = 1\n",
        "        for yp in sorted(ysd.keys(), reverse=True):\n",
        "            if np.sum(ysd[yp]) > 0:\n",
        "                rankofs = 1 \n",
        "                # rankofs = 1 - np.sum(ysd[yp]) / len(ysd[yp])\n",
        "                rank += len(ysd[yp]) * rankofs\n",
        "            rank += len(ysd[yp])\n",
        "            temp.append(float(counter) / float(1+rank)) # change to rank\n",
        "            counter += 1\n",
        "        temp_mean = np.mean(temp)\n",
        " \n",
        "        rr.append(temp_mean)\n",
        " \n",
        "    return np.mean(rr)\n",
        "\n",
        "\n",
        "AnsSelRes = namedtuple('AnsSelRes', ['MRR', 'MAP'])\n",
        "\n",
        "def eval_QA(pred, q, y):\n",
        "    mrr_ = mrr(q, y, pred)\n",
        "    \n",
        "    print('MRR: %f' % (mrr_))\n",
        "\n",
        "    map_ = mapcalc(q, y, pred)\n",
        "\n",
        "    print('MAP: %f' % (map_))\n",
        "    return AnsSelRes(mrr_, map_)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Task-specific callbacks for the fit() function.\n",
        "\"\"\"\n",
        "\n",
        "class AnsSelCB(Callback):\n",
        "    \"\"\" A callback that monitors answer selection validation ACC after each epoch \"\"\"\n",
        "    def __init__(self, val_q, val_s, y, inputs):\n",
        "        self.val_q = val_q\n",
        "        self.val_s = val_s\n",
        "        self.val_y = y \n",
        "        self.val_inputs = inputs\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        pred = self.model.predict(self.val_inputs)        \n",
        "        mrr_ = mrr(self.val_q, self.val_y, pred)\n",
        "        map_ = mapcalc(self.val_q, self.val_y, pred)\n",
        "        print('val MRR %f' % (mrr_,))\n",
        "        logs['mrr'] = mrr_\n",
        "        print('val MAP %f' % (map_,))\n",
        "        logs['map'] = map_\n",
        "\n"
      ],
      "metadata": {
        "id": "KA19RfxtVAjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Config"
      ],
      "metadata": {
        "id": "3uoGcoVbiYay"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwHt_lKAb8iZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7324f5-fd61-4655-8bea-c6ae04e7f050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import sys,os\n",
        "\n",
        "import csv\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import keras.activations as activations\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Input, TimeDistributed, BatchNormalization\n",
        "from keras.layers.merge import concatenate, add, multiply, dot\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
        "from keras.layers.core import Activation, Dense, Dropout, Flatten, Lambda, Permute, RepeatVector\n",
        "from keras.layers.recurrent import GRU, LSTM\n",
        "from keras.layers import CuDNNGRU, CuDNNLSTM, Bidirectional, MultiHeadAttention\n",
        "\n",
        "from keras import backend as K\n",
        "from tensorflow.compat.v1.keras.backend import set_session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = tf.ConfigProto()\n",
        "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
        "config.gpu_options.allow_growth = True\n",
        "set_session(tf.Session(config=config))"
      ],
      "metadata": {
        "id": "61PBFKzLHDQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Pu1LbUpKb8ic"
      },
      "outputs": [],
      "source": [
        "def config():\n",
        "    c = dict()\n",
        "    # embedding params\n",
        "    c['emb'] = 'Glove'\n",
        "    c['embdim'] = 300    # change to 300 \n",
        "    c['inp_e_dropout'] = 1/2\n",
        "    c['flag'] = True\n",
        "    c['pe'] = True\n",
        "    c['pe_method'] = 'fixed' # 'fixed' or 'learned'\n",
        "\n",
        "    # training hyperparams\n",
        "    c['opt'] = 'adam'\n",
        "    c['batch_size'] = 320\n",
        "    c['epochs'] = 5 # change to 160\n",
        "    c['patience'] = 155\n",
        "    \n",
        "    # sentences with word lengths below the 'pad' will be padded with 0.\n",
        "    c['pad'] = 60\n",
        "    \n",
        "    # rnn model       \n",
        "    c['rnn_dropout'] = 1/2     \n",
        "    c['l2reg'] = 1e-4\n",
        "                                              \n",
        "    c['rnnbidi'] = True                      \n",
        "    c['rnn'] = CuDNNLSTM\n",
        "    c['rnnbidi_mode'] = concatenate\n",
        "    c['rnnact'] = 'tanh'\n",
        "    c['rnninit'] = 'glorot_uniform'                      \n",
        "    c['sdim'] = 5\n",
        "\n",
        "    # cnn model\n",
        "    c['cnn_dropout'] = 1/2     \n",
        "    c['pool_layer'] = MaxPooling1D\n",
        "    c['cnnact'] = 'relu'\n",
        "    c['cnninit'] = 'glorot_uniform'\n",
        "    c['pact'] = 'tanh'\n",
        "\n",
        "    # projection layer\n",
        "    c['proj'] = True\n",
        "    c['pdim'] = 1/2\n",
        "    c['p_layers'] = 1\n",
        "    c['p_dropout'] = 1/2\n",
        "    c['p_init'] = 'glorot_uniform'\n",
        "    \n",
        "    # QA-LSTM/CNN+attention\n",
        "    c['adim'] = 1/2\n",
        "    c['cfiltlen'] = 3\n",
        "    \n",
        "    # Attentive Pooling-LSTM/CNN\n",
        "    c['w_feat_model'] = 'rnn'\n",
        "    c['bll_dropout'] = 1/2\n",
        "    \n",
        "    # self attention model\n",
        "    c['self_pdim'] = 1/2\n",
        "\n",
        "    # mlp scoring function\n",
        "    c['Ddim'] = 2\n",
        "    \n",
        "    ps, h = hash_params(c)\n",
        "\n",
        "    return c, ps, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ysgx1rixb8ie"
      },
      "outputs": [],
      "source": [
        "conf = None\n",
        "emb = None\n",
        "vocab = None\n",
        "inp_tr = None\n",
        "inp_val = None\n",
        "inp_test = None\n",
        "y_val = None\n",
        "y_test = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mQ4t_SXkb8ig"
      },
      "outputs": [],
      "source": [
        "def ranknet(y_true, y_pred):\n",
        "    return K.mean(K.log(1. + K.exp(-(y_true * y_pred - (1-y_true) * y_pred))), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnvHodkEb8ih"
      },
      "source": [
        "## Data Load\n",
        "\n",
        "Load TrecQA dataset (wang et al. 2007).  http://cs.stanford.edu/people/mengqiu/data/qg-emnlp07-data.tgz\n",
        "\n",
        "The format of the dataset is as follows.\n",
        "- question1, label, sentence1   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qJVQoH2fb8ii"
      },
      "outputs": [],
      "source": [
        "def load_data_from_file(dsfile):\n",
        "    #load a dataset in the csv format;\n",
        "    q = [] # a set of questions\n",
        "    sents = [] # a set of sentences\n",
        "    labels = [] # a set of labels\n",
        "\n",
        "    with open(dsfile) as f:\n",
        "        c = csv.DictReader(f)\n",
        "        for l in c:\n",
        "            label = int(l['label'])\n",
        "            labels.append(label)\n",
        "            try:\n",
        "                qtext = l['qtext'].decode('utf8')\n",
        "                stext = l['atext'].decode('utf8')\n",
        "            except AttributeError:  # python3 has no .decode()\n",
        "                qtext = l['qtext']\n",
        "                stext = l['atext']\n",
        "            \n",
        "            q.append(qtext.split(' '))\n",
        "            sents.append(stext.split(' '))\n",
        "            \n",
        "    return (q, sents, labels)\n",
        "    \n",
        "def make_model_inputs(qi, si, f01, f10, q, sents, y):\n",
        "    inp = {'qi': qi, 'si': si, 'f01':f01, 'f10':f10, 'q':q, 'sents':sents, 'y':y} \n",
        "    \n",
        "    return inp\n",
        "\n",
        "def load_set(fname, vocab=None, iseval=False):\n",
        "    q, sents, y = load_data_from_file(fname)\n",
        "    if not iseval:\n",
        "        vocab = Vocabulary(q + sents) \n",
        "    \n",
        "    pad = conf['pad']\n",
        "    \n",
        "    qi = vocab.vectorize(q, pad=pad)  \n",
        "    si = vocab.vectorize(sents, pad=pad)        \n",
        "    f01, f10 = sentence_flags(q, sents, pad)  \n",
        "    \n",
        "    inp = make_model_inputs(qi, si, f01, f10, q, sents, y)\n",
        "    if iseval:\n",
        "        return (inp, y)\n",
        "    else:\n",
        "        return (inp, y, vocab)        \n",
        "    \n",
        "def load_data(trainf, valf, testf):\n",
        "    global vocab, inp_tr, inp_val, inp_test, y_train, y_val, y_test\n",
        "    inp_tr, y_train, vocab = load_set(trainf, iseval=False)\n",
        "    inp_val, y_val = load_set(valf, vocab=vocab, iseval=True)\n",
        "    inp_test, y_test = load_set(testf, vocab=vocab, iseval=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Individual Encoding:"
      ],
      "metadata": {
        "id": "1Ayb-Ueoa1z2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0dkJL12tb8ij"
      },
      "outputs": [],
      "source": [
        "def embedding():\n",
        "    '''\n",
        "    Declare all inputs (vectorized sentences and NLP flags)\n",
        "    and generate outputs representing vector sequences with dropout applied.  \n",
        "    Returns the vector dimensionality.       \n",
        "    '''\n",
        "    pad = conf['pad']\n",
        "    dropout = conf['inp_e_dropout']\n",
        "    \n",
        "    # story selection\n",
        "    input_qi = Input(name='qi', shape=(pad,), dtype='int32')                          \n",
        "    input_si = Input(name='si', shape=(pad,), dtype='int32')                 \n",
        "    input_f01 = Input(name='f01', shape=(pad, flagsdim))\n",
        "    input_f10 = Input(name='f10', shape=(pad, flagsdim))         \n",
        "\n",
        "    if conf['flag']:\n",
        "        input_nodes = [input_qi, input_si, input_f01, input_f10]\n",
        "        N = emb.N + flagsdim\n",
        "    else:\n",
        "        input_nodes = [input_qi, input_si]\n",
        "        N = emb.N\n",
        "\n",
        "    shared_embedding = Embedding(name='emb', input_dim=vocab.size(), input_length=pad,\n",
        "                                output_dim=emb.N, mask_zero=False,\n",
        "                                weights=[vocab.embmatrix(emb)], trainable=True)\n",
        "    # nlp flag\n",
        "    if conf['flag']:\n",
        "        emb_qi = concatenate([shared_embedding(input_qi), input_f01])\n",
        "        emb_si = concatenate([shared_embedding(input_si), input_f10])\n",
        "    else:\n",
        "        emb_qi = shared_embedding(input_qi)\n",
        "        emb_si = shared_embedding(input_si)\n",
        "    \n",
        "    # positional encoding\n",
        "    if conf['pe']:\n",
        "        if conf['pe_method'] == 'fixed':\n",
        "            encoding = position_encoding_fixed(pad, N)\n",
        "            # pe_layer = Lambda(name='pe_fixed_layer', \n",
        "            #         function=lambda x: batch_multiply(x, encoding), \n",
        "            #         output_shape=lambda shape:shape)\n",
        "            emb_qi = batch_multiply(emb_qi, encoding)\n",
        "            emb_si = batch_multiply(emb_si, encoding)\n",
        "        elif conf['pe_method'] == 'learned':\n",
        "            encoder = Embedding(name='pe_learnable_layer', input_dim=conf['pad'], input_length=conf['pad'],\n",
        "                                            output_dim=304, mask_zero=False, trainable=True)\n",
        "            pos_val = K.constant(value=np.arange(conf['pad'])) # shape=(pad,)\n",
        "            pos_val = K.expand_dims(pos_val, axis=0) # shape=(1, pad)\n",
        "            pos_val = K.tile(pos_val, (K.shape(input_qi)[0], 1)) # shape=(batch_size_of_x, pad)\n",
        "            pos_input = Input(name='pos_input', tensor=pos_val)\n",
        "            input_nodes.append(pos_input)\n",
        "            encoding = encoder(pos_input)\n",
        "\n",
        "            emb_qi = add([emb_qi, encoding])\n",
        "            emb_si = add([emb_si, encoding])\n",
        "    \n",
        "    emb_qi = Dropout(dropout, noise_shape=(None, pad, N))(emb_qi)\n",
        "    emb_si = Dropout(dropout, noise_shape=(None, pad, N))(emb_si) # shape=(None, pad, N)\n",
        "\n",
        "    emb_outputs = [emb_qi, emb_si]\n",
        "    \n",
        "    return N, input_nodes, emb_outputs\n",
        "\n",
        "def batch_multiply(x, y): \n",
        "    y = K.expand_dims(y, axis=0)\n",
        "    y = K.tile(y, (K.shape(x)[0], 1, 1)) \n",
        "    return multiply([x, y]) \n",
        "\n",
        "def position_encoding_fixed(sentence_size, embedding_size):\n",
        "    \"\"\" \n",
        "    Position Encoding described in https://arxiv.org/pdf/1503.08895.pdf\n",
        "    \"\"\"\n",
        "    encoding = np.ones((embedding_size, sentence_size), dtype=np.float32)\n",
        "    ls = sentence_size + 1\n",
        "    le = embedding_size + 1\n",
        "    for i in range(1, le):\n",
        "        for j in range(1, ls):\n",
        "            encoding[i-1, j-1] = (i - (embedding_size+1)/2) * (j - (sentence_size+1)/2)\n",
        "    encoding = 1 + 4 * encoding / embedding_size / sentence_size\n",
        "    # Make position encoding of time words identity to avoid modifying them \n",
        "    encoding[:, -1] = 1.0\n",
        "    # encoding = K.variable(value=np.transpose(encoding)) # shape=(pad, N)\n",
        "    temp = np.transpose(encoding)\n",
        "    encoding = tf.convert_to_tensor(temp, dtype=tf.float32)\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combined Encoding (with Tests)"
      ],
      "metadata": {
        "id": "XqbnSYTl2G5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7QJcq2wz2G5T"
      },
      "outputs": [],
      "source": [
        "def combined_embedding():\n",
        "    '''\n",
        "    Declare all inputs (vectorized sentences and NLP flags)\n",
        "    and generate outputs representing vector sequences with dropout applied.  \n",
        "    Returns the vector dimensionality.       \n",
        "    '''\n",
        "    pad = conf['pad']\n",
        "    dropout = conf['inp_e_dropout']\n",
        "    \n",
        "    # story selection\n",
        "    input_qi = Input(name='qi', shape=(pad,), dtype='int32')                          \n",
        "    input_si = Input(name='si', shape=(pad,), dtype='int32')                 \n",
        "    input_f01 = Input(name='f01', shape=(pad, flagsdim))\n",
        "    input_f10 = Input(name='f10', shape=(pad, flagsdim))         \n",
        "\n",
        "    if conf['flag']:\n",
        "        input_nodes = [input_qi, input_si, input_f01, input_f10]\n",
        "        N = emb.N + flagsdim\n",
        "    else:\n",
        "        input_nodes = [input_qi, input_si]\n",
        "        N = emb.N\n",
        "\n",
        "    shared_embedding = Embedding(name='emb', input_dim=vocab.size(), input_length=pad,\n",
        "                                output_dim=emb.N, mask_zero=False,\n",
        "                                weights=[vocab.embmatrix(emb)], trainable=True)\n",
        "    # nlp flag\n",
        "    if conf['flag']:\n",
        "        emb_qi = concatenate([shared_embedding(input_qi), input_f01])\n",
        "        emb_si = concatenate([shared_embedding(input_si), input_f10])\n",
        "    else:\n",
        "        emb_qi = shared_embedding(input_qi)\n",
        "        emb_si = shared_embedding(input_si)\n",
        "    \n",
        "    # positional encoding\n",
        "    if conf['pe']:\n",
        "        # Fixed PE\n",
        "        encoding_fixed = position_encoding_fixed(pad, N)\n",
        "        emb_qi_fixed = batch_multiply(emb_qi, encoding_fixed)\n",
        "        emb_si_fixed = batch_multiply(emb_qi, encoding_fixed)\n",
        "\n",
        "        # Trainable PE\n",
        "        encoder_learn = Embedding(name='pe_learnable_layer', input_dim=conf['pad'], input_length=conf['pad'],\n",
        "                                        output_dim=304, mask_zero=False, trainable=True)\n",
        "        pos_val = K.constant(value=np.arange(conf['pad'])) # shape=(pad,)\n",
        "        pos_val = K.expand_dims(pos_val, axis=0) # shape=(1, pad)\n",
        "        pos_val = K.tile(pos_val, (K.shape(input_qi)[0], 1)) # shape=(batch_size_of_x, pad)\n",
        "        pos_input = Input(name='pos_input', tensor=pos_val)\n",
        "        input_nodes.append(pos_input)\n",
        "        encoding_learn = encoder_learn(pos_input)\n",
        "        emb_qi_learn = add([emb_qi, encoding_learn])\n",
        "        emb_si_learn = add([emb_si, encoding_learn])\n",
        "\n",
        "        print(emb_qi_fixed.shape)\n",
        "        print(emb_si_fixed.shape)\n",
        "        print(emb_qi_learn.shape)\n",
        "        print(emb_qi_learn.shape)\n",
        "\n",
        "        # # Uncomment when combining\n",
        "        emb_qi= add([emb_qi_fixed, emb_qi_learn])\n",
        "        emb_si = add([emb_si_fixed, emb_si_learn])\n",
        "\n",
        "        emb_qi = Dropout(dropout, noise_shape=(None, pad, N))(emb_qi)\n",
        "        emb_si = Dropout(dropout, noise_shape=(None, pad, N))(emb_si) # shape=(None, pad, N)\n",
        "\n",
        "        emb_qi = Permute((2,1))(emb_qi)\n",
        "        emb_si = Permute((2,1))(emb_si)\n",
        "        emb_qi = Activation('tanh')(Bidirectional(LSTM(60, return_sequences=True), merge_mode='ave')(emb_qi))\n",
        "        emb_si = Activation('tanh')(Bidirectional(LSTM(60, return_sequences=True), merge_mode='ave')(emb_si))\n",
        "        emb_qi = Permute((2,1))(emb_qi)\n",
        "        emb_si = Permute((2,1))(emb_si)\n",
        "    \n",
        "    emb_outputs = [emb_qi, emb_si]\n",
        "    \n",
        "    return N, input_nodes, emb_outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Test:"
      ],
      "metadata": {
        "id": "Jy8zNPQDs86d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/gdrive/MyDrive/FYP_Project/data'\n",
        "trainf = path + '/train-all.csv' \n",
        "valf = path + '/dev.csv'\n",
        "testf = path + '/test.csv'\n",
        "params = []\n",
        "\n",
        "conf, ps, h = config()\n",
        "\n",
        "if conf['emb'] == 'Glove': # Please download the GloVe in here http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    print('GloVe')\n",
        "    emb = GloVe(N=conf['embdim'])\n",
        "\n",
        "print('Dataset')\n",
        "load_data(trainf,valf,testf)\n",
        "N, input_nodes_emb, output_nodes_emb = combined_embedding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "ix1aaMrutArV",
        "outputId": "27ebfbe3-858a-41ca-9630-2074708cdc2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ff62e57d2c5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emb'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Glove'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Please download the GloVe in here http://nlp.stanford.edu/data/glove.6B.zip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GloVe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGloVe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embdim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-675a84a5295c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, N, glovepath)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# with open('glove.6B.300d.txt') as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglovepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(N)\n",
        "print(len(input_nodes_emb))\n",
        "print(input_nodes_emb[0].shape)\n",
        "print(input_nodes_emb[1].shape)\n",
        "print(input_nodes_emb[2].shape)\n",
        "print(input_nodes_emb[3].shape)\n",
        "print(input_nodes_emb[4].shape)\n",
        "print(len(output_nodes_emb))\n",
        "print(output_nodes_emb[0].shape)\n",
        "print(output_nodes_emb[1].shape)"
      ],
      "metadata": {
        "id": "kkHH7cl9u1PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Utils"
      ],
      "metadata": {
        "id": "RuCXJ_NWeAIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def projection_layer(inputs, input_size):\n",
        "    input0 = inputs[0]\n",
        "    input1 = inputs[1]\n",
        "    for p_i in range(conf['p_layers']):\n",
        "        shared_dense = Dense(name='pdeep%d'%(p_i), units=int(input_size*conf['pdim']),\n",
        "                activation='linear', kernel_initializer=conf['p_init'], kernel_regularizer=l2(conf['l2reg']))\n",
        "        qi_proj = Activation(conf['pact'])(BatchNormalization()(shared_dense(input0)))\n",
        "        si_proj = Activation(conf['pact'])(BatchNormalization()(shared_dense(input1)))\n",
        "        input0 = qi_proj\n",
        "        input1 = si_proj\n",
        "        input_size = int(input_size * conf['pdim'])\n",
        "\n",
        "    dropout = conf['p_dropout']\n",
        "    qi_proj = Dropout(dropout, noise_shape=(input_size,))(qi_proj)\n",
        "    si_proj = Dropout(dropout, noise_shape=(input_size,))(si_proj)\n",
        "\n",
        "    return qi_proj, si_proj"
      ],
      "metadata": {
        "id": "77TpJvgieCti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average Model"
      ],
      "metadata": {
        "id": "RgQXCqL_zAy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_model(input_nodes, N, pfx=''):\n",
        "    shared_dense = Dense(int(N), activation='linear', name='wproj'+pfx)\n",
        "    qi_wproj = TimeDistributed(shared_dense)(input_nodes[0])\n",
        "    si_wproj = TimeDistributed(shared_dense)(input_nodes[1])\n",
        "    \n",
        "    qi_wproj = TimeDistributed(BatchNormalization())(qi_wproj)\n",
        "    si_wproj = TimeDistributed(BatchNormalization())(si_wproj)\n",
        "\n",
        "    qi_wproj = TimeDistributed(Activation('tanh'))(qi_wproj)\n",
        "    si_wproj = TimeDistributed(Activation('tanh'))(si_wproj)\n",
        "    \n",
        "    avg_layer = Lambda(name='bow'+pfx, function=lambda x: K.mean(x, axis=1), output_shape=lambda shape:(shape[0],) + shape[2:])\n",
        "    qi_avg = avg_layer(qi_wproj)\n",
        "    si_avg = avg_layer(si_wproj)\n",
        "\n",
        "    if conf['proj']:\n",
        "        qi_avg, si_avg = projection_layer([qi_avg, si_avg], int(N))\n",
        "\n",
        "    return [qi_avg, si_avg]"
      ],
      "metadata": {
        "id": "zJr98GSzzDbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISz5of2Eb8ip"
      },
      "source": [
        "## RNN model (with Tests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-hdmzLI6b8iq"
      },
      "outputs": [],
      "source": [
        "def rnn_model(input_nodes, N, pfx=''):\n",
        "    qi_rnn, si_rnn, nc = rnn_input(N, pfx=pfx, dropout=conf['rnn_dropout'], sdim=conf['sdim'], \n",
        "                            rnnbidi_mode=conf['rnnbidi_mode'], rnn=conf['rnn'], rnnact=conf['rnnact'], \n",
        "                            rnninit=conf['rnninit'], inputs=input_nodes, return_sequence=False)\n",
        "\n",
        "    if conf['proj']:\n",
        "        qi_rnn, si_rnn = projection_layer([qi_rnn, si_rnn], nc)\n",
        "    \n",
        "    print(qi_rnn.shape)\n",
        "    print(si_rnn.shape)\n",
        "    return [qi_rnn, si_rnn]\n",
        "\n",
        "def rnn_input(N, dropout=3/4, sdim=2, rnn=GRU, rnnact='tanh', rnninit='glorot_uniform', rnnbidi_mode=add, \n",
        "              inputs=None, return_sequence=True, pfx=''):\n",
        "    if rnnbidi_mode == concatenate:\n",
        "        sdim /= 2\n",
        "    shared_rnn_f = rnn(int(N*sdim), kernel_initializer=rnninit, input_shape=(None, conf['pad'], N), \n",
        "                       return_sequences=return_sequence, name='rnnf'+pfx)\n",
        "    shared_rnn_b = rnn(int(N*sdim), kernel_initializer=rnninit, input_shape=(None, conf['pad'], N),\n",
        "                       return_sequences=return_sequence, go_backwards=True, name='rnnb'+pfx)\n",
        "    qi_rnn_f = shared_rnn_f(inputs[0])\n",
        "    si_rnn_f = shared_rnn_f(inputs[1])\n",
        "    \n",
        "    qi_rnn_b = shared_rnn_b(inputs[0])\n",
        "    si_rnn_b = shared_rnn_b(inputs[1])\n",
        "    \n",
        "    qi_rnn = Activation(rnnact)(BatchNormalization()(rnnbidi_mode([qi_rnn_f, qi_rnn_b])))\n",
        "    si_rnn = Activation(rnnact)(BatchNormalization()(rnnbidi_mode([si_rnn_f, si_rnn_b])))\n",
        "    \n",
        "    if rnnbidi_mode == concatenate:\n",
        "        sdim *= 2\n",
        "        \n",
        "    qi_rnn = Dropout(dropout, noise_shape=(int(N*sdim),))(qi_rnn)\n",
        "    si_rnn = Dropout(dropout, noise_shape=(int(N*sdim),))(si_rnn)\n",
        "    \n",
        "    return (qi_rnn, si_rnn, int(N*sdim))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test RNN Model"
      ],
      "metadata": {
        "id": "Yvi-JPsF-hv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_emb = []\n",
        "N, input_nodes_emb, output_nodes_emb = embedding() \n",
        "iam = rnn_model(output_nodes_emb, N, pfx= 'S')\n",
        "test_model = Model(inputs=input_nodes_emb, outputs=iam)\n",
        "test_model.summary()"
      ],
      "metadata": {
        "id": "Bqy6_Z6m-hcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGV_IV5Tb8iq"
      },
      "source": [
        "## CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BvolNVtub8ir"
      },
      "outputs": [],
      "source": [
        "def cnn_model(input_nodes, N, pfx=''):\n",
        "    qi_cnn, si_cnn, nc = cnnsum_input(conf['pad'], dropout=conf['cnn_dropout'],\n",
        "                                l2reg=conf['l2reg'], cnninit=conf['cnninit'], cnnact=conf['cnnact'],\n",
        "                                input_dim=N, inputs=input_nodes)\n",
        "    if conf['proj']:\n",
        "        qi_cnn, si_cnn = projection_layer([qi_cnn, si_cnn], nc)\n",
        "\n",
        "    return [qi_cnn, si_cnn]\n",
        "\n",
        "def cnnsum_input(pad, dropout=3/4, l2reg=1e-4, cnninit='glorot_uniform', cnnact='relu',\n",
        "        cdim={1: 1/2, 2: 1/2, 3: 1/2, 4: 1/2, 5: 1/2, 6: 1/2, 7: 1/2}, inputs=None, input_dim=304, pfx=''):\n",
        "    qi_cnn_res_list = []\n",
        "    si_cnn_res_list = []\n",
        "    tot_len = 0\n",
        "    for fl, cd in cdim.items():\n",
        "        nb_filter = int(input_dim*cd)\n",
        "        shared_conv = Convolution1D(name=pfx+'conv%d'%(fl), input_shape=(None, conf['pad'], input_dim),\n",
        "                    kernel_size=fl, filters=nb_filter, activation='linear',\n",
        "                    kernel_regularizer=l2(l2reg), kernel_initializer=cnninit)\n",
        "        qi_cnn_one = Activation(cnnact)(BatchNormalization()(shared_conv(inputs[0])))\n",
        "        si_cnn_one = Activation(cnnact)(BatchNormalization()(shared_conv(inputs[1])))\n",
        "        \n",
        "        pool = MaxPooling1D(pool_size=int(conf['pad']-fl+1), name=pfx+'pool%d'%(fl))\n",
        "        qi_pool_one = pool(qi_cnn_one)\n",
        "        si_pool_one = pool(si_cnn_one)\n",
        "\n",
        "        flatten = Flatten(name=pfx+'flatten%d'%(fl))\n",
        "        qi_out_one = flatten(qi_pool_one)\n",
        "        si_out_one = flatten(si_pool_one)\n",
        "\n",
        "        qi_cnn_res_list.append(qi_out_one)\n",
        "        si_cnn_res_list.append(si_out_one)\n",
        "    \n",
        "        tot_len += nb_filter\n",
        "\n",
        "    qi_cnn = Dropout(dropout, noise_shape=(tot_len,))(concatenate(qi_cnn_res_list))\n",
        "    si_cnn = Dropout(dropout, noise_shape=(tot_len,))(concatenate(si_cnn_res_list))\n",
        "\n",
        "    return (qi_cnn, si_cnn, tot_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Attention Model (with Tests)"
      ],
      "metadata": {
        "id": "exOfbj5eg4Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attention_model(input_nodes, N, pfx=''):\n",
        "    # only using self-attention\n",
        "    # apply point-wise fc-layer on both-side (1x1 spatial convolution)\n",
        "    shared_dense = Dense(int(conf['self_pdim']*N), activation='linear', name='pont_wise_fc'+pfx) \n",
        "    qi_key = TimeDistributed(shared_dense)(input_nodes[0])\n",
        "    si_key = TimeDistributed(shared_dense)(input_nodes[1]) \n",
        "\n",
        "    qi_key = TimeDistributed(BatchNormalization())(qi_key)\n",
        "    si_key = TimeDistributed(BatchNormalization())(si_key)\n",
        "\n",
        "    qi_key = TimeDistributed(Activation('relu'))(qi_key)\n",
        "    si_key = TimeDistributed(Activation('relu'))(si_key)\n",
        "    \n",
        "    # one-more 1x1 spartial convolution\n",
        "    shared_dense_attn = Dense(1, activation='linear', name='point_wise_fc_attn'+pfx) \n",
        "    qi_matching = TimeDistributed(shared_dense_attn)(qi_key)\n",
        "    si_matching = TimeDistributed(shared_dense_attn)(si_key)\n",
        "    \n",
        "    # get attn values\n",
        "    flatten = Flatten(name='attn_flatten'+pfx)\n",
        "    qi_matching = Activation('softmax')(flatten(qi_matching))\n",
        "    qi_matching = RepeatVector(int(N))(qi_matching)\n",
        "    qi_matching = Permute((2,1))(qi_matching)\n",
        "    si_matching = Activation('softmax')(flatten(si_matching))\n",
        "    si_matching = RepeatVector(int(N))(si_matching)\n",
        "    si_matching = Permute((2,1))(si_matching)\n",
        "\n",
        "    # q, sentence updates\n",
        "    qi_val = multiply([qi_matching, input_nodes[0]])\n",
        "    si_val = multiply([si_matching, input_nodes[1]])\n",
        "\n",
        "    # weighted_averaging\n",
        "    avg_layer = Lambda(name='bow'+pfx, function=lambda x: K.mean(x, axis=1), output_shape=lambda shape:(shape[0],) + shape[2:])\n",
        "    qi_val = avg_layer(qi_val)\n",
        "    si_val = avg_layer(si_val)\n",
        "\n",
        "    if conf['proj']:\n",
        "        qi_val, si_val = projection_layer([qi_val, si_val], int(N))\n",
        "\n",
        "    print(qi_val.shape)\n",
        "    print(si_val.shape)\n",
        "    return [qi_val, si_val]"
      ],
      "metadata": {
        "id": "SA9iCI9sg8J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Self Attention Module"
      ],
      "metadata": {
        "id": "J_Y7GDsy9m2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_emb = []\n",
        "N, input_nodes_emb, output_nodes_emb = embedding() \n",
        "iam = self_attention_model(output_nodes_emb, N, pfx= 'S')\n",
        "test_model = Model(inputs=input_nodes_emb, outputs=iam)\n",
        "test_model.summary()"
      ],
      "metadata": {
        "id": "s9nFBNuM9GFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "364b9bc7-3f34-41b3-a32e-f806b36656de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(?, 304)\n",
            "(?, 304)\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " qi (InputLayer)                [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " si (InputLayer)                [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " emb (Embedding)                (None, 60, 300)      15508200    ['qi[0][0]',                     \n",
            "                                                                  'si[0][0]']                     \n",
            "                                                                                                  \n",
            " f01 (InputLayer)               [(None, 60, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " f10 (InputLayer)               [(None, 60, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 60, 304)      0           ['emb[0][0]',                    \n",
            "                                                                  'f01[0][0]']                    \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 60, 304)      0           ['emb[1][0]',                    \n",
            "                                                                  'f10[0][0]']                    \n",
            "                                                                                                  \n",
            " tf_op_layer_Shape_8 (TensorFlo  [(3,)]              0           ['concatenate_16[0][0]']         \n",
            " wOpLayer)                                                                                        \n",
            "                                                                                                  \n",
            " tf_op_layer_Shape_9 (TensorFlo  [(3,)]              0           ['concatenate_17[0][0]']         \n",
            " wOpLayer)                                                                                        \n",
            "                                                                                                  \n",
            " tf_op_layer_strided_slice_8 (T  [()]                0           ['tf_op_layer_Shape_8[0][0]']    \n",
            " ensorFlowOpLayer)                                                                                \n",
            "                                                                                                  \n",
            " tf_op_layer_strided_slice_9 (T  [()]                0           ['tf_op_layer_Shape_9[0][0]']    \n",
            " ensorFlowOpLayer)                                                                                \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_8/multiples (  [(3,)]              0           ['tf_op_layer_strided_slice_8[0][\n",
            " TensorFlowOpLayer)                                              0]']                             \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_9/multiples (  [(3,)]              0           ['tf_op_layer_strided_slice_9[0][\n",
            " TensorFlowOpLayer)                                              0]']                             \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_8 (TensorFlow  [(None, 60, 304)]   0           ['tf_op_layer_Tile_8/multiples[0]\n",
            " OpLayer)                                                        [0]']                            \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_9 (TensorFlow  [(None, 60, 304)]   0           ['tf_op_layer_Tile_9/multiples[0]\n",
            " OpLayer)                                                        [0]']                            \n",
            "                                                                                                  \n",
            " multiply_10 (Multiply)         (None, 60, 304)      0           ['concatenate_16[0][0]',         \n",
            "                                                                  'tf_op_layer_Tile_8[0][0]']     \n",
            "                                                                                                  \n",
            " multiply_11 (Multiply)         (None, 60, 304)      0           ['concatenate_17[0][0]',         \n",
            "                                                                  'tf_op_layer_Tile_9[0][0]']     \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 60, 304)      0           ['multiply_10[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 60, 304)      0           ['multiply_11[0][0]']            \n",
            "                                                                                                  \n",
            " time_distributed_18 (TimeDistr  (None, 60, 152)     46360       ['dropout_13[0][0]']             \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " time_distributed_19 (TimeDistr  (None, 60, 152)     46360       ['dropout_14[0][0]']             \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " time_distributed_20 (TimeDistr  (None, 60, 152)     608         ['time_distributed_18[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " time_distributed_21 (TimeDistr  (None, 60, 152)     608         ['time_distributed_19[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " time_distributed_22 (TimeDistr  (None, 60, 152)     0           ['time_distributed_20[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " time_distributed_23 (TimeDistr  (None, 60, 152)     0           ['time_distributed_21[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " time_distributed_24 (TimeDistr  (None, 60, 1)       153         ['time_distributed_22[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " time_distributed_25 (TimeDistr  (None, 60, 1)       153         ['time_distributed_23[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " attn_flattenS (Flatten)        (None, 60)           0           ['time_distributed_24[0][0]',    \n",
            "                                                                  'time_distributed_25[0][0]']    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 60)           0           ['attn_flattenS[0][0]']          \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 60)           0           ['attn_flattenS[1][0]']          \n",
            "                                                                                                  \n",
            " repeat_vector (RepeatVector)   (None, 304, 60)      0           ['activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " repeat_vector_1 (RepeatVector)  (None, 304, 60)     0           ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " permute_18 (Permute)           (None, 60, 304)      0           ['repeat_vector[0][0]']          \n",
            "                                                                                                  \n",
            " permute_19 (Permute)           (None, 60, 304)      0           ['repeat_vector_1[0][0]']        \n",
            "                                                                                                  \n",
            " multiply_12 (Multiply)         (None, 60, 304)      0           ['permute_18[0][0]',             \n",
            "                                                                  'dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " multiply_13 (Multiply)         (None, 60, 304)      0           ['permute_19[0][0]',             \n",
            "                                                                  'dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " bowS (Lambda)                  (None, 304)          0           ['multiply_12[0][0]',            \n",
            "                                                                  'multiply_13[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15,555,929\n",
            "Trainable params: 15,555,321\n",
            "Non-trainable params: 608\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive Attention (with Tests)"
      ],
      "metadata": {
        "id": "sg1Kpk8nlX2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_attention_model(input_nodes, N, pfx=''):\n",
        "    # d=304\n",
        "    # m=n=60\n",
        "    shared_dense = Dense(int(N), activation='linear', name='wproj'+pfx)\n",
        "    qi_wproj = TimeDistributed(shared_dense)(input_nodes[0])\n",
        "    si_wproj = TimeDistributed(shared_dense)(input_nodes[1])\n",
        "    \n",
        "    qi_wproj = TimeDistributed(BatchNormalization())(qi_wproj)\n",
        "    si_wproj = TimeDistributed(BatchNormalization())(si_wproj)\n",
        "\n",
        "    qi_wproj = TimeDistributed(Activation('sigmoid'))(qi_wproj)\n",
        "    si_wproj = TimeDistributed(Activation('sigmoid'))(si_wproj)\n",
        "\n",
        "    qi_e = Permute((2,1))(qi_wproj) # Shape is now ?xdxn\n",
        "    qi_e = BatchNormalization()(qi_e)\n",
        "    si_e = Permute((2,1))(si_wproj) # Shape is now ?xdxm\n",
        "    si_e = BatchNormalization()(si_e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 1 Alignment :)\n",
        "    qi_e_trans = Permute((2,1))(qi_e) # Shape is now ?xnxd\n",
        "    qi_e_trans = BatchNormalization()(qi_e_trans)\n",
        "    alignment = dot(axes=(2,1), inputs=[qi_e_trans,si_e]) # Shape is now ?xnxm\n",
        "    alignment = BatchNormalization()(alignment)\n",
        "    alignment_trans = Permute((2,1))(alignment) # Shape is now ?xmxn\n",
        "    alignment_trans = BatchNormalization()(alignment_trans)\n",
        "    r_a = Activation('softmax')(alignment) # Shape is now ?xnxm\n",
        "    r_q = Activation('softmax')(alignment_trans) # Shape is now ?xmxn\n",
        "    print(r_a.shape)\n",
        "    print(r_q.shape)\n",
        "\n",
        "    \n",
        "    # 2 Co-Attention :)\n",
        "    q_sum = dot(axes=(2,1), inputs=[qi_e, r_a]) # Shape is now ?xdxm\n",
        "    q_sum = BatchNormalization()(q_sum)\n",
        "    a_sum = dot(axes=(2,1), inputs=[si_e, r_q]) # Shape is now ?xdxn\n",
        "    a_sum = BatchNormalization()(a_sum)\n",
        "\n",
        "    q_coa = dot(axes=(2,1), inputs=[q_sum,r_q]) # Shape is now ?xdxn\n",
        "    q_coa = BatchNormalization()(q_coa)\n",
        "    a_coa = dot(axes=(2,1), inputs=[a_sum,r_a]) # Shape is now ?xdxm   \n",
        "    a_coa = BatchNormalization()(a_coa)\n",
        "    print(q_coa.shape)\n",
        "    print(a_coa.shape)\n",
        "\n",
        "\n",
        "    # 3 Representation Compression :)\n",
        "    q_full = concatenate([qi_e, a_sum, q_coa]) # Shape is now ?xdx3n\n",
        "    a_full = concatenate([si_e, q_sum, a_coa]) # Shape is now ?xdx3m\n",
        "    print(q_full.shape)\n",
        "    print(a_full.shape)\n",
        "    q_com = Activation('sigmoid')(Bidirectional(LSTM(30, return_sequences=True), merge_mode='concat')(q_full)) # Shape is now ?xdxn\n",
        "    q_com = BatchNormalization()(q_com) \n",
        "\n",
        "    a_com = Activation('sigmoid')(Bidirectional(LSTM(30, return_sequences=True), merge_mode='concat')(a_full)) # Shape is now ?xdxm\n",
        "    a_com = BatchNormalization()(a_com) \n",
        "\n",
        "    print(q_com.shape)\n",
        "    print(a_com.shape)\n",
        "\n",
        "\n",
        "    # 4 TF Self-Attention :)\n",
        "    # q_com = concatenate(axis=1, inputs=[q_com, q_com, q_com, q_com])\n",
        "    # a_com = concatenate(axis=1, inputs=[a_com, a_com, a_com, a_com])\n",
        "    mha = MultiHeadAttention(key_dim=100, value_dim=100, num_heads=4) \n",
        "    # mha = BatchNormalization()(mha) \n",
        "\n",
        "    q_mha, q_attn = mha(q_com, q_com, return_attention_scores=True) # Shape is now ?xdxm\n",
        "    a_mha, a_attn = mha(a_com, a_com, return_attention_scores=True) # Shape is now ?xdxm\n",
        "    print(q_mha.shape)\n",
        "    print(a_mha.shape)\n",
        "\n",
        "    # Residual FFN :)\n",
        "    q_res = Dense(60, activation='relu')(q_mha)\n",
        "    q_res = BatchNormalization()(q_res) \n",
        "\n",
        "    a_res = Dense(60, activation='relu')(a_mha)\n",
        "    a_res = BatchNormalization()(a_res) \n",
        "\n",
        "    q_self = add([q_res, q_mha]) # Shape is now ?xdxm\n",
        "    a_self = add([a_res, a_mha]) # Shape is now ?xdxm\n",
        "    print(q_self.shape)\n",
        "    print(a_self.shape)\n",
        "\n",
        "    # 5 Aggregation :)\n",
        "    q_final = Activation('tanh')(Bidirectional(LSTM(60, return_sequences=True), merge_mode='ave')(q_self))\n",
        "    q_final = BatchNormalization()(q_final) \n",
        "    a_final = Activation('tanh')(Bidirectional(LSTM(60, return_sequences=True), merge_mode='ave')(a_self))\n",
        "    a_final = BatchNormalization()(a_final) \n",
        "\n",
        "    print(q_final.shape)\n",
        "    print(a_final.shape)\n",
        "\n",
        "    qi_val = Permute((2,1))(q_final) # Shape is now ?xdxn\n",
        "    qi_val = BatchNormalization()(qi_val) \n",
        "\n",
        "    si_val = Permute((2,1))(a_final) # Shape is now ?xdxm\n",
        "    si_val = BatchNormalization()(si_val) \n",
        "\n",
        "    # 6 Weighted_averaging :)\n",
        "    avg_layer = Lambda(name='bow'+pfx, function=lambda x: K.mean(x, axis=1), output_shape=lambda shape:(shape[0],) + shape[2:])\n",
        "    qi_val = avg_layer(qi_val)\n",
        "    si_val = avg_layer(si_val)\n",
        "\n",
        "    if conf['proj']:\n",
        "        qi_val, si_val = projection_layer([qi_val, si_val], int(N))\n",
        "\n",
        "    print(\"qi_val.shape: \",qi_val.shape)\n",
        "    print(\"si_val.shape: \",si_val.shape)\n",
        "\n",
        "\n",
        "    return [qi_val, si_val]"
      ],
      "metadata": {
        "id": "onQQwiUBlaeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Interactive Attention Module\n",
        "Call Embedding and Data Load Before this in 'Run Code' section"
      ],
      "metadata": {
        "id": "VryulIYYvfU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_emb = []\n",
        "N, input_nodes_emb, output_nodes_emb = embedding() \n",
        "conf['proj']= False\n",
        "iam = interactive_attention_model(output_nodes_emb, N, pfx= 'S')\n",
        "test_model = Model(inputs=input_nodes_emb, outputs=iam)\n",
        "test_model.summary()"
      ],
      "metadata": {
        "id": "UH-J1AwWvi6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d614bf65-3aa3-4f2e-c045-73714eb0049e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(?, 60, 60)\n",
            "(?, 60, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 180)\n",
            "(?, 304, 180)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "qi_val.shape:  (?, 304)\n",
            "si_val.shape:  (?, 304)\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " qi (InputLayer)                [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " emb (Embedding)                (None, 60, 300)      15508200    ['qi[0][0]',                     \n",
            "                                                                  'si[0][0]']                     \n",
            "                                                                                                  \n",
            " f01 (InputLayer)               [(None, 60, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 60, 304)      0           ['emb[0][0]',                    \n",
            "                                                                  'f01[0][0]']                    \n",
            "                                                                                                  \n",
            " si (InputLayer)                [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_op_layer_Shape_4 (TensorFlo  [(3,)]              0           ['concatenate_10[0][0]']         \n",
            " wOpLayer)                                                                                        \n",
            "                                                                                                  \n",
            " f10 (InputLayer)               [(None, 60, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " tf_op_layer_strided_slice_4 (T  [()]                0           ['tf_op_layer_Shape_4[0][0]']    \n",
            " ensorFlowOpLayer)                                                                                \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 60, 304)      0           ['emb[1][0]',                    \n",
            "                                                                  'f10[0][0]']                    \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_4/multiples (  [(3,)]              0           ['tf_op_layer_strided_slice_4[0][\n",
            " TensorFlowOpLayer)                                              0]']                             \n",
            "                                                                                                  \n",
            " tf_op_layer_Shape_5 (TensorFlo  [(3,)]              0           ['concatenate_11[0][0]']         \n",
            " wOpLayer)                                                                                        \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_4 (TensorFlow  [(None, 60, 304)]   0           ['tf_op_layer_Tile_4/multiples[0]\n",
            " OpLayer)                                                        [0]']                            \n",
            "                                                                                                  \n",
            " tf_op_layer_strided_slice_5 (T  [()]                0           ['tf_op_layer_Shape_5[0][0]']    \n",
            " ensorFlowOpLayer)                                                                                \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 60, 304)      0           ['concatenate_10[0][0]',         \n",
            "                                                                  'tf_op_layer_Tile_4[0][0]']     \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_5/multiples (  [(3,)]              0           ['tf_op_layer_strided_slice_5[0][\n",
            " TensorFlowOpLayer)                                              0]']                             \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 60, 304)      0           ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_5 (TensorFlow  [(None, 60, 304)]   0           ['tf_op_layer_Tile_5/multiples[0]\n",
            " OpLayer)                                                        [0]']                            \n",
            "                                                                                                  \n",
            " time_distributed_12 (TimeDistr  (None, 60, 304)     92720       ['dropout_8[0][0]']              \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " multiply_7 (Multiply)          (None, 60, 304)      0           ['concatenate_11[0][0]',         \n",
            "                                                                  'tf_op_layer_Tile_5[0][0]']     \n",
            "                                                                                                  \n",
            " time_distributed_14 (TimeDistr  (None, 60, 304)     1216        ['time_distributed_12[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 60, 304)      0           ['multiply_7[0][0]']             \n",
            "                                                                                                  \n",
            " time_distributed_16 (TimeDistr  (None, 60, 304)     0           ['time_distributed_14[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " time_distributed_13 (TimeDistr  (None, 60, 304)     92720       ['dropout_9[0][0]']              \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " permute_12 (Permute)           (None, 304, 60)      0           ['time_distributed_16[0][0]']    \n",
            "                                                                                                  \n",
            " time_distributed_15 (TimeDistr  (None, 60, 304)     1216        ['time_distributed_13[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 304, 60)     240         ['permute_12[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " time_distributed_17 (TimeDistr  (None, 60, 304)     0           ['time_distributed_15[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " permute_13 (Permute)           (None, 304, 60)      0           ['time_distributed_17[0][0]']    \n",
            "                                                                                                  \n",
            " permute_14 (Permute)           (None, 60, 304)      0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 304, 60)     240         ['permute_13[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 60, 304)     1216        ['permute_14[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dot_10 (Dot)                   (None, 60, 60)       0           ['batch_normalization_44[0][0]', \n",
            "                                                                  'batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 60, 60)      240         ['dot_10[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " permute_15 (Permute)           (None, 60, 60)       0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 60, 60)      240         ['permute_15[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 60, 60)       0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 60, 60)       0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " dot_12 (Dot)                   (None, 304, 60)      0           ['batch_normalization_43[0][0]', \n",
            "                                                                  'activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " dot_11 (Dot)                   (None, 304, 60)      0           ['batch_normalization_42[0][0]', \n",
            "                                                                  'activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 304, 60)     240         ['dot_12[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 304, 60)     240         ['dot_11[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dot_13 (Dot)                   (None, 304, 60)      0           ['batch_normalization_47[0][0]', \n",
            "                                                                  'activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " dot_14 (Dot)                   (None, 304, 60)      0           ['batch_normalization_48[0][0]', \n",
            "                                                                  'activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 304, 60)     240         ['dot_13[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 304, 60)     240         ['dot_14[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 304, 180)     0           ['batch_normalization_42[0][0]', \n",
            "                                                                  'batch_normalization_48[0][0]', \n",
            "                                                                  'batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 304, 180)     0           ['batch_normalization_43[0][0]', \n",
            "                                                                  'batch_normalization_47[0][0]', \n",
            "                                                                  'batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " bidirectional_8 (Bidirectional  (None, 304, 60)     50640       ['concatenate_12[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_9 (Bidirectional  (None, 304, 60)     50640       ['concatenate_13[0][0]']         \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 304, 60)      0           ['bidirectional_8[0][0]']        \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 304, 60)      0           ['bidirectional_9[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 304, 60)     240         ['activation_24[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_52 (BatchN  (None, 304, 60)     240         ['activation_25[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  ((None, 304, 60),   97260       ['batch_normalization_51[0][0]', \n",
            " eadAttention)                   (None, 4, 304, 304               'batch_normalization_51[0][0]', \n",
            "                                ))                                'batch_normalization_52[0][0]', \n",
            "                                                                  'batch_normalization_52[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 304, 60)      3660        ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 304, 60)      3660        ['multi_head_attention_2[1][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_53 (BatchN  (None, 304, 60)     240         ['dense_4[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_54 (BatchN  (None, 304, 60)     240         ['dense_5[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 304, 60)      0           ['batch_normalization_53[0][0]', \n",
            "                                                                  'multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 304, 60)      0           ['batch_normalization_54[0][0]', \n",
            "                                                                  'multi_head_attention_2[1][0]'] \n",
            "                                                                                                  \n",
            " bidirectional_10 (Bidirectiona  (None, 304, 60)     58080       ['add_6[0][0]']                  \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " bidirectional_11 (Bidirectiona  (None, 304, 60)     58080       ['add_7[0][0]']                  \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 304, 60)      0           ['bidirectional_10[0][0]']       \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 304, 60)      0           ['bidirectional_11[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_55 (BatchN  (None, 304, 60)     240         ['activation_26[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_56 (BatchN  (None, 304, 60)     240         ['activation_27[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " permute_16 (Permute)           (None, 60, 304)      0           ['batch_normalization_55[0][0]'] \n",
            "                                                                                                  \n",
            " permute_17 (Permute)           (None, 60, 304)      0           ['batch_normalization_56[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_57 (BatchN  (None, 60, 304)     1216        ['permute_16[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_58 (BatchN  (None, 60, 304)     1216        ['permute_17[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " bowS (Lambda)                  (None, 304)          0           ['batch_normalization_57[0][0]', \n",
            "                                                                  'batch_normalization_58[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15,932,380\n",
            "Trainable params: 15,927,660\n",
            "Non-trainable params: 4,720\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling with Self Attention (with Tests)"
      ],
      "metadata": {
        "id": "lJtVVxVieXZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ap_self_attention_model"
      ],
      "metadata": {
        "id": "QuicZL9BnAfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ap_self_attention_model(input_nodes, N, pfx=''):\n",
        "    # only using self-attention\n",
        "    # apply point-wise fc-layer on both-side (1x1 spatial convolution)\n",
        "    shared_dense = Dense(int(conf['self_pdim']*N), activation='linear', name='pont_wise_fc'+pfx) \n",
        "    qi_key = TimeDistributed(shared_dense)(input_nodes[0])\n",
        "    si_key = TimeDistributed(shared_dense)(input_nodes[1]) \n",
        "\n",
        "    qi_key = TimeDistributed(BatchNormalization())(qi_key)\n",
        "    si_key = TimeDistributed(BatchNormalization())(si_key)\n",
        "\n",
        "    qi_key = TimeDistributed(Activation('relu'))(qi_key)\n",
        "    si_key = TimeDistributed(Activation('relu'))(si_key)\n",
        "    \n",
        "    # one-more 1x1 spartial convolution\n",
        "    shared_dense_attn = Dense(1, activation='linear', name='point_wise_fc_attn'+pfx) \n",
        "    qi_matching = TimeDistributed(shared_dense_attn)(qi_key)\n",
        "    si_matching = TimeDistributed(shared_dense_attn)(si_key)\n",
        "    \n",
        "    # get attn values\n",
        "    flatten = Flatten(name='attn_flatten'+pfx)\n",
        "    qi_matching = Activation('softmax')(flatten(qi_matching))\n",
        "    qi_matching = RepeatVector(int(N))(qi_matching)\n",
        "    qi_matching = Permute((2,1))(qi_matching)\n",
        "    si_matching = Activation('softmax')(flatten(si_matching))\n",
        "    si_matching = RepeatVector(int(N))(si_matching)\n",
        "    si_matching = Permute((2,1))(si_matching)\n",
        "\n",
        "    # q, sentence updates\n",
        "    qi_val = multiply([qi_matching, input_nodes[0]])\n",
        "    si_val = multiply([si_matching, input_nodes[1]])\n",
        "\n",
        "    # weighted_averaging\n",
        "    # avg_layer = Lambda(name='bow'+pfx, function=lambda x: K.mean(x, axis=1), output_shape=lambda shape:(shape[0],) + shape[2:])\n",
        "    # qi_val = avg_layer(qi_val)\n",
        "    # si_val = avg_layer(si_val)\n",
        "\n",
        "    print(qi_val.shape)\n",
        "    print(si_val.shape)\n",
        "\n",
        "    if conf['proj']:\n",
        "        qi_val, si_val = projection_layer([qi_val, si_val], int(N))\n",
        "    \n",
        "    print(qi_val.shape)\n",
        "    print(si_val.shape)\n",
        "\n",
        "    return [qi_val, si_val]"
      ],
      "metadata": {
        "id": "5YknrtoEm9Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attentive Pooling (with Tests)"
      ],
      "metadata": {
        "id": "gbxRNpApPNoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ap_model(input_nodes, N, pfx=''):\n",
        "        qi_feat, si_feat = ap_self_attention_model(N=N, pfx='sa_q'+pfx, input_nodes=input_nodes)\n",
        "        \"\"\"Attentive pooling\n",
        "        Args:\n",
        "            q: encoder output for question (batch_size, q_len, vector_size)\n",
        "            a: encoder output for question (batch_size, a_len, vector_size)\n",
        "        Returns:\n",
        "            final representation Tensor r_q, r_a for q and a (batch_size, vector_size)\n",
        "        \"\"\"\n",
        "        batch_size = conf['batch_size']\n",
        "        # c = q.get_shape().as_list()[-1]  # vector size\n",
        "        c = 304\n",
        "        m = 60\n",
        "        n = 60\n",
        "\n",
        "        \n",
        "        # G = tanh(Q*U*A^T)  here Q is equal to Q transpose in origin paper.\n",
        "        Q = qi_feat  # (b, m, c)\n",
        "        A = si_feat  # (b, n, c)\n",
        "        print(\"Q.shape: \", Q.shape)\n",
        "        print(\"A.shape: \", A.shape)\n",
        "        temp = np.zeros((c,c))\n",
        "        # U = tf.Variable(tf.truncated_normal([c,c], stddev=0.05))\n",
        "        U = tf.convert_to_tensor(temp, dtype=tf.float32)\n",
        "        U_batch = K.tile(K.expand_dims(U, axis=0), (K.shape(Q)[0], 1, 1))\n",
        "        print(Q.shape)\n",
        "        print(U_batch.shape)\n",
        "        print(A.shape)\n",
        "        G = tf.tanh(\n",
        "            tf.matmul(\n",
        "                tf.matmul(Q, U_batch), A, transpose_b=True)\n",
        "        ) \n",
        "        print(\"G.shape:\",G.shape) # G b*m*n\n",
        "\n",
        "        # column-wise and row-wise max-poolings to generate g_q (b*m*1), g_a (b*1*n)\n",
        "        g_q = tf.reduce_max(G, axis=2, keepdims=True)\n",
        "        g_a = tf.reduce_max(G, axis=1, keepdims=True)\n",
        "        print(\"g_q.shape:\", g_q.shape)\n",
        "        print(\"g_a.shape:\",g_a.shape)\n",
        "\n",
        "        # create attention vectors sigma_q (b*m), sigma_a (b*n)\n",
        "        sigma_q = tf.nn.softmax(g_q)\n",
        "        sigma_a = tf.nn.softmax(g_a)\n",
        "        # final output r_q, r_a  (b*c)\n",
        "        r_q = tf.squeeze(tf.matmul(tf.transpose(Q, [0, 2, 1]), sigma_q), axis=2)\n",
        "        r_a = tf.squeeze(tf.matmul(sigma_a, A), axis=1)\n",
        "        print(\"r_q.shape:\", r_q.shape)\n",
        "        print(\"r_a.shape:\",r_a.shape)\n",
        "\n",
        "        return [r_q, r_a]  # (b, c)\n"
      ],
      "metadata": {
        "id": "hh8EJRh5PQvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N, input_nodes_emb, output_nodes_emb = embedding()\n",
        "conf['proj'] = False\n",
        "iam = ap_model(output_nodes_emb, N, pfx= 'S')\n",
        "test_model = Model(inputs=input_nodes_emb, outputs=iam)\n",
        "test_model.summary()\n",
        "test_model.compile(loss=ranknet, optimizer=conf['opt'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "0OVhEIn0PRGY",
        "outputId": "444dd929-c672-468b-98f1-d66d998deaf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-c11217e587fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_nodes_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_nodes_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'proj'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0miam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0map_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_nodes_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpfx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_nodes_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-4f5bc2c2a4ab>\u001b[0m in \u001b[0;36membedding\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvector\u001b[0m \u001b[0mdimensionality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     '''\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pad'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inp_e_dropout'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max Pooling (with Tests)"
      ],
      "metadata": {
        "id": "BWgFv3bUeBgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maxp_model(input_nodes, N, pfx=''):\n",
        "    qi_feat, si_feat = ap_self_attention_model(N=N, pfx='sa_q'+pfx, input_nodes=input_nodes)\n",
        "    # apply column wise max pooling (correct)\n",
        "    maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))\n",
        "    # apply row wise max pooling ??\n",
        "    # maxpool = Lambda(lambda x: K.max(x, axis=2, keepdims=False), output_shape=lambda x: (x[0], x[1]))\n",
        "\n",
        "    maxpool.supports_masking = True\n",
        "    question_pool = maxpool(qi_feat)\n",
        "    answer_pool = maxpool(si_feat)\n",
        "    print(\"question_pool.shape\", question_pool.shape)\n",
        "    print(\"answer_pool.shape\", answer_pool.shape)\n",
        "\n",
        "    return [question_pool, answer_pool]"
      ],
      "metadata": {
        "id": "6IJXzShseDow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N, input_nodes_emb, output_nodes_emb = embedding()\n",
        "conf['proj'] = False\n",
        "iam = maxp_model(output_nodes_emb, N, pfx= 'S')\n",
        "test_model = Model(inputs=input_nodes_emb, outputs=iam)\n",
        "test_model.summary()\n",
        "test_model.compile(loss=ranknet, optimizer=conf['opt'])"
      ],
      "metadata": {
        "id": "YvjA_0W1ed2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avg Pooling (with Tests)"
      ],
      "metadata": {
        "id": "_mdKyAQ0kkn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avgp_model(input_nodes, N, pfx=''):\n",
        "    qi_feat, si_feat = ap_self_attention_model(N=N, pfx='sa_q'+pfx, input_nodes=input_nodes)\n",
        "    # apply column wise mean pooling (correct)\n",
        "    meanpool = Lambda(lambda x: K.mean(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))\n",
        "    # apply row wise mean pooling ??\n",
        "    # meanpool = Lambda(lambda x: K.mean(x, axis=2, keepdims=False), output_shape=lambda x: (x[0], x[1]))\n",
        "\n",
        "    meanpool.supports_masking = True\n",
        "    question_pool = meanpool(qi_feat)\n",
        "    answer_pool = meanpool(si_feat)\n",
        "    print(\"question_pool.shape\", question_pool.shape)\n",
        "    print(\"answer_pool.shape\", answer_pool.shape)\n",
        "\n",
        "    return [question_pool, answer_pool]"
      ],
      "metadata": {
        "id": "d4riYPk9kowF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N, input_nodes_emb, output_nodes_emb = embedding()\n",
        "conf['proj'] = False\n",
        "iam = avgp_model(output_nodes_emb, N, pfx= 'S')\n",
        "test_model = Model(inputs=input_nodes_emb, outputs=iam)\n",
        "test_model.summary()\n",
        "test_model.compile(loss=ranknet, optimizer=conf['opt'])"
      ],
      "metadata": {
        "id": "OAaOScWpko9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling with Interactive Attention (with Tests)"
      ],
      "metadata": {
        "id": "pLqvwCvZncaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ap_interactive_attention_model"
      ],
      "metadata": {
        "id": "4wHEw2mzncaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ap_interactive_attention_model(input_nodes, N, pfx=''):\n",
        "    # d=304\n",
        "    # m=n=60\n",
        "    shared_dense = Dense(int(N), activation='linear', name='wproj'+pfx)\n",
        "    qi_wproj = TimeDistributed(shared_dense)(input_nodes[0])\n",
        "    si_wproj = TimeDistributed(shared_dense)(input_nodes[1])\n",
        "    \n",
        "    qi_wproj = TimeDistributed(BatchNormalization())(qi_wproj)\n",
        "    si_wproj = TimeDistributed(BatchNormalization())(si_wproj)\n",
        "\n",
        "    qi_wproj = TimeDistributed(Activation('sigmoid'))(qi_wproj)\n",
        "    si_wproj = TimeDistributed(Activation('sigmoid'))(si_wproj)\n",
        "\n",
        "    qi_e = Permute((2,1))(qi_wproj) # Shape is now ?xdxn\n",
        "    qi_e = BatchNormalization()(qi_e)\n",
        "    si_e = Permute((2,1))(si_wproj) # Shape is now ?xdxm\n",
        "    si_e = BatchNormalization()(si_e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 1 Alignment :)\n",
        "    qi_e_trans = Permute((2,1))(qi_e) # Shape is now ?xnxd\n",
        "    qi_e_trans = BatchNormalization()(qi_e_trans)\n",
        "    alignment = dot(axes=(2,1), inputs=[qi_e_trans,si_e]) # Shape is now ?xnxm\n",
        "    alignment = BatchNormalization()(alignment)\n",
        "    alignment_trans = Permute((2,1))(alignment) # Shape is now ?xmxn\n",
        "    alignment_trans = BatchNormalization()(alignment_trans)\n",
        "    r_a = Activation('softmax')(alignment) # Shape is now ?xnxm\n",
        "    r_q = Activation('softmax')(alignment_trans) # Shape is now ?xmxn\n",
        "    print(r_a.shape)\n",
        "    print(r_q.shape)\n",
        "\n",
        "    \n",
        "    # 2 Co-Attention :)\n",
        "    q_sum = dot(axes=(2,1), inputs=[qi_e, r_a]) # Shape is now ?xdxm\n",
        "    q_sum = BatchNormalization()(q_sum)\n",
        "    a_sum = dot(axes=(2,1), inputs=[si_e, r_q]) # Shape is now ?xdxn\n",
        "    a_sum = BatchNormalization()(a_sum)\n",
        "\n",
        "    q_coa = dot(axes=(2,1), inputs=[q_sum,r_q]) # Shape is now ?xdxn\n",
        "    q_coa = BatchNormalization()(q_coa)\n",
        "    a_coa = dot(axes=(2,1), inputs=[a_sum,r_a]) # Shape is now ?xdxm   \n",
        "    a_coa = BatchNormalization()(a_coa)\n",
        "    print(q_coa.shape)\n",
        "    print(a_coa.shape)\n",
        "\n",
        "\n",
        "    # 3 Representation Compression :)\n",
        "    q_full = concatenate([qi_e, a_sum, q_coa]) # Shape is now ?xdx3n\n",
        "    a_full = concatenate([si_e, q_sum, a_coa]) # Shape is now ?xdx3m\n",
        "    print(q_full.shape)\n",
        "    print(a_full.shape)\n",
        "    q_com = Activation('sigmoid')(Bidirectional(LSTM(30, return_sequences=True), merge_mode='concat')(q_full)) # Shape is now ?xdxn\n",
        "    q_com = BatchNormalization()(q_com) \n",
        "\n",
        "    a_com = Activation('sigmoid')(Bidirectional(LSTM(30, return_sequences=True), merge_mode='concat')(a_full)) # Shape is now ?xdxm\n",
        "    a_com = BatchNormalization()(a_com) \n",
        "\n",
        "    print(q_com.shape)\n",
        "    print(a_com.shape)\n",
        "\n",
        "\n",
        "    # 4 TF Self-Attention :)\n",
        "    # q_com = concatenate(axis=1, inputs=[q_com, q_com, q_com, q_com])\n",
        "    # a_com = concatenate(axis=1, inputs=[a_com, a_com, a_com, a_com])\n",
        "    mha = MultiHeadAttention(key_dim=100, value_dim=100, num_heads=4) \n",
        "    # mha = BatchNormalization()(mha) \n",
        "\n",
        "    q_mha, q_attn = mha(q_com, q_com, return_attention_scores=True) # Shape is now ?xdxm\n",
        "    a_mha, a_attn = mha(a_com, a_com, return_attention_scores=True) # Shape is now ?xdxm\n",
        "    print(q_mha.shape)\n",
        "    print(a_mha.shape)\n",
        "\n",
        "    # Residual FFN :)\n",
        "    q_res = Dense(60, activation='relu')(q_mha)\n",
        "    q_res = BatchNormalization()(q_res) \n",
        "\n",
        "    a_res = Dense(60, activation='relu')(a_mha)\n",
        "    a_res = BatchNormalization()(a_res) \n",
        "\n",
        "    q_self = add([q_res, q_mha]) # Shape is now ?xdxm\n",
        "    a_self = add([a_res, a_mha]) # Shape is now ?xdxm\n",
        "    print(q_self.shape)\n",
        "    print(a_self.shape)\n",
        "\n",
        "    # 5 Aggregation :)\n",
        "    q_final = Activation('sigmoid')(Bidirectional(LSTM(60, return_sequences=True), merge_mode='ave')(q_self))\n",
        "    q_final = BatchNormalization()(q_final) \n",
        "    a_final = Activation('sigmoid')(Bidirectional(LSTM(60, return_sequences=True), merge_mode='ave')(a_self))\n",
        "    a_final = BatchNormalization()(a_final) \n",
        "\n",
        "    print(q_final.shape)\n",
        "    print(a_final.shape)\n",
        "\n",
        "    qi_val = Permute((2,1))(q_final) # Shape is now ?xdxn\n",
        "    qi_val = BatchNormalization()(qi_val) \n",
        "\n",
        "    si_val = Permute((2,1))(a_final) # Shape is now ?xdxm\n",
        "    si_val = BatchNormalization()(si_val) \n",
        "\n",
        "    # # 6 Weighted_averaging :)\n",
        "    # avg_layer = Lambda(name='bow'+pfx, function=lambda x: K.mean(x, axis=1), output_shape=lambda shape:(shape[0],) + shape[2:])\n",
        "    # qi_val = avg_layer(qi_val)\n",
        "    # si_val = avg_layer(si_val)\n",
        "\n",
        "    if conf['proj']:\n",
        "        qi_val, si_val = projection_layer([qi_val, si_val], int(N))\n",
        "\n",
        "    print(\"qi_val.shape: \",qi_val.shape)\n",
        "    print(\"si_val.shape: \",si_val.shape)\n",
        "\n",
        "\n",
        "    return [qi_val, si_val]"
      ],
      "metadata": {
        "id": "pi96AguPncaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attentive Pooling (with Tests)"
      ],
      "metadata": {
        "id": "iuhBwtrnncaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ap_ia_model(input_nodes, N, pfx=''):\n",
        "        qi_feat, si_feat = ap_interactive_attention_model(N=N, pfx='sa_q'+pfx, input_nodes=input_nodes)\n",
        "        \"\"\"Attentive pooling\n",
        "        Args:\n",
        "            q: encoder output for question (batch_size, q_len, vector_size)\n",
        "            a: encoder output for question (batch_size, a_len, vector_size)\n",
        "        Returns:\n",
        "            final representation Tensor r_q, r_a for q and a (batch_size, vector_size)\n",
        "        \"\"\"\n",
        "        batch_size = conf['batch_size']\n",
        "        # c = q.get_shape().as_list()[-1]  # vector size\n",
        "        c = 304\n",
        "        m = 60\n",
        "        n = 60\n",
        "\n",
        "        \n",
        "        # G = tanh(Q*U*A^T)  here Q is equal to Q transpose in origin paper.\n",
        "        Q = qi_feat  # (b, m, c)\n",
        "        A = si_feat  # (b, n, c)\n",
        "        print(\"Q.shape: \", Q.shape)\n",
        "        print(\"A.shape: \", A.shape)\n",
        "        temp = np.zeros((c,c))\n",
        "        # U = tf.Variable(tf.truncated_normal([c,c], stddev=0.05))\n",
        "        U = tf.convert_to_tensor(temp, dtype=tf.float32)\n",
        "        U_batch = K.tile(K.expand_dims(U, axis=0), (K.shape(Q)[0], 1, 1))\n",
        "        print(Q.shape)\n",
        "        print(U_batch.shape)\n",
        "        print(A.shape)\n",
        "        G = tf.tanh(\n",
        "            tf.matmul(\n",
        "                tf.matmul(Q, U_batch), A, transpose_b=True)\n",
        "        ) \n",
        "        print(\"G.shape:\",G.shape) # G b*m*n\n",
        "\n",
        "        # column-wise and row-wise max-poolings to generate g_q (b*m*1), g_a (b*1*n)\n",
        "        g_q = tf.reduce_max(G, axis=2, keepdims=True)\n",
        "        g_a = tf.reduce_max(G, axis=1, keepdims=True)\n",
        "        print(\"g_q.shape:\", g_q.shape)\n",
        "        print(\"g_a.shape:\",g_a.shape)\n",
        "\n",
        "        # create attention vectors sigma_q (b*m), sigma_a (b*n)\n",
        "        sigma_q = tf.nn.softmax(g_q)\n",
        "        sigma_a = tf.nn.softmax(g_a)\n",
        "        # final output r_q, r_a  (b*c)\n",
        "        r_q = tf.squeeze(tf.matmul(tf.transpose(Q, [0, 2, 1]), sigma_q), axis=2)\n",
        "        r_a = tf.squeeze(tf.matmul(sigma_a, A), axis=1)\n",
        "        print(\"r_q.shape:\", r_q.shape)\n",
        "        print(\"r_a.shape:\",r_a.shape)\n",
        "\n",
        "        return [r_q, r_a]  # (b, c)\n"
      ],
      "metadata": {
        "id": "lpyQ_tivncaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N, input_nodes_emb, output_nodes_emb = embedding()\n",
        "conf['proj'] = False\n",
        "iam = ap_ia_model(output_nodes_emb, N, pfx= 'S')\n",
        "test_model = Model(inputs=input_nodes_emb, outputs=iam)\n",
        "test_model.summary()\n",
        "test_model.compile(loss=ranknet, optimizer=conf['opt'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de44040-bbed-4fe5-f124-fbf3f4100f57",
        "id": "vl0EI_QzncaX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(?, 60, 60)\n",
            "(?, 60, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 180)\n",
            "(?, 304, 180)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "qi_val.shape:  (?, 60, 304)\n",
            "si_val.shape:  (?, 60, 304)\n",
            "Q.shape:  (?, 60, 304)\n",
            "A.shape:  (?, 60, 304)\n",
            "(?, 60, 304)\n",
            "(?, 304, 304)\n",
            "(?, 60, 304)\n",
            "G.shape: (?, 60, 60)\n",
            "g_q.shape: (?, 60, 1)\n",
            "g_a.shape: (?, 1, 60)\n",
            "r_q.shape: (?, 304)\n",
            "r_a.shape: (?, 304)\n",
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " qi (InputLayer)                [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " emb (Embedding)                (None, 60, 300)      15508200    ['qi[0][0]',                     \n",
            "                                                                  'si[0][0]']                     \n",
            "                                                                                                  \n",
            " f01 (InputLayer)               [(None, 60, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " pos_input (InputLayer)         [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " concatenate_42 (Concatenate)   (None, 60, 304)      0           ['emb[0][0]',                    \n",
            "                                                                  'f01[0][0]']                    \n",
            "                                                                                                  \n",
            " pe_learnable_layer (Embedding)  (None, 60, 304)     18240       ['pos_input[0][0]']              \n",
            "                                                                                                  \n",
            " si (InputLayer)                [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " add_40 (Add)                   (None, 60, 304)      0           ['concatenate_42[0][0]',         \n",
            "                                                                  'pe_learnable_layer[0][0]']     \n",
            "                                                                                                  \n",
            " f10 (InputLayer)               [(None, 60, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)           (None, 60, 304)      0           ['add_40[0][0]']                 \n",
            "                                                                                                  \n",
            " concatenate_43 (Concatenate)   (None, 60, 304)      0           ['emb[1][0]',                    \n",
            "                                                                  'f10[0][0]']                    \n",
            "                                                                                                  \n",
            " time_distributed_54 (TimeDistr  (None, 60, 304)     92720       ['dropout_26[0][0]']             \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " add_41 (Add)                   (None, 60, 304)      0           ['concatenate_43[0][0]',         \n",
            "                                                                  'pe_learnable_layer[0][0]']     \n",
            "                                                                                                  \n",
            " time_distributed_56 (TimeDistr  (None, 60, 304)     1216        ['time_distributed_54[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)           (None, 60, 304)      0           ['add_41[0][0]']                 \n",
            "                                                                                                  \n",
            " time_distributed_58 (TimeDistr  (None, 60, 304)     0           ['time_distributed_56[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " time_distributed_55 (TimeDistr  (None, 60, 304)     92720       ['dropout_27[0][0]']             \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " permute_52 (Permute)           (None, 304, 60)      0           ['time_distributed_58[0][0]']    \n",
            "                                                                                                  \n",
            " time_distributed_57 (TimeDistr  (None, 60, 304)     1216        ['time_distributed_55[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_151 (Batch  (None, 304, 60)     240         ['permute_52[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " time_distributed_59 (TimeDistr  (None, 60, 304)     0           ['time_distributed_57[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " permute_53 (Permute)           (None, 304, 60)      0           ['time_distributed_59[0][0]']    \n",
            "                                                                                                  \n",
            " permute_54 (Permute)           (None, 60, 304)      0           ['batch_normalization_151[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_152 (Batch  (None, 304, 60)     240         ['permute_53[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_153 (Batch  (None, 60, 304)     1216        ['permute_54[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dot_45 (Dot)                   (None, 60, 60)       0           ['batch_normalization_153[0][0]',\n",
            "                                                                  'batch_normalization_152[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_154 (Batch  (None, 60, 60)      240         ['dot_45[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " permute_55 (Permute)           (None, 60, 60)       0           ['batch_normalization_154[0][0]']\n",
            "                                                                                                  \n",
            " activation_78 (Activation)     (None, 60, 60)       0           ['batch_normalization_154[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_155 (Batch  (None, 60, 60)      240         ['permute_55[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dot_46 (Dot)                   (None, 304, 60)      0           ['batch_normalization_151[0][0]',\n",
            "                                                                  'activation_78[0][0]']          \n",
            "                                                                                                  \n",
            " activation_79 (Activation)     (None, 60, 60)       0           ['batch_normalization_155[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_156 (Batch  (None, 304, 60)     240         ['dot_46[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dot_47 (Dot)                   (None, 304, 60)      0           ['batch_normalization_152[0][0]',\n",
            "                                                                  'activation_79[0][0]']          \n",
            "                                                                                                  \n",
            " dot_48 (Dot)                   (None, 304, 60)      0           ['batch_normalization_156[0][0]',\n",
            "                                                                  'activation_79[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_157 (Batch  (None, 304, 60)     240         ['dot_47[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_158 (Batch  (None, 304, 60)     240         ['dot_48[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " concatenate_44 (Concatenate)   (None, 304, 180)     0           ['batch_normalization_151[0][0]',\n",
            "                                                                  'batch_normalization_157[0][0]',\n",
            "                                                                  'batch_normalization_158[0][0]']\n",
            "                                                                                                  \n",
            " bidirectional_34 (Bidirectiona  (None, 304, 60)     50640       ['concatenate_44[0][0]']         \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " activation_80 (Activation)     (None, 304, 60)      0           ['bidirectional_34[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_160 (Batch  (None, 304, 60)     240         ['activation_80[0][0]']          \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dot_49 (Dot)                   (None, 304, 60)      0           ['batch_normalization_157[0][0]',\n",
            "                                                                  'activation_78[0][0]']          \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (MultiH  ((None, 304, 60),   97260       ['batch_normalization_160[0][0]',\n",
            " eadAttention)                   (None, 4, 304, 304               'batch_normalization_160[0][0]',\n",
            "                                ))                                'batch_normalization_161[0][0]',\n",
            "                                                                  'batch_normalization_161[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_159 (Batch  (None, 304, 60)     240         ['dot_49[0][0]']                 \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 304, 60)      3660        ['multi_head_attention_9[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_45 (Concatenate)   (None, 304, 180)     0           ['batch_normalization_152[0][0]',\n",
            "                                                                  'batch_normalization_156[0][0]',\n",
            "                                                                  'batch_normalization_159[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_162 (Batch  (None, 304, 60)     240         ['dense_16[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " bidirectional_35 (Bidirectiona  (None, 304, 60)     50640       ['concatenate_45[0][0]']         \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " add_42 (Add)                   (None, 304, 60)      0           ['batch_normalization_162[0][0]',\n",
            "                                                                  'multi_head_attention_9[0][0]'] \n",
            "                                                                                                  \n",
            " activation_81 (Activation)     (None, 304, 60)      0           ['bidirectional_35[0][0]']       \n",
            "                                                                                                  \n",
            " bidirectional_36 (Bidirectiona  (None, 304, 60)     58080       ['add_42[0][0]']                 \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_161 (Batch  (None, 304, 60)     240         ['activation_81[0][0]']          \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation_82 (Activation)     (None, 304, 60)      0           ['bidirectional_36[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_164 (Batch  (None, 304, 60)     240         ['activation_82[0][0]']          \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 304, 60)      3660        ['multi_head_attention_9[1][0]'] \n",
            "                                                                                                  \n",
            " permute_56 (Permute)           (None, 60, 304)      0           ['batch_normalization_164[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_163 (Batch  (None, 304, 60)     240         ['dense_17[0][0]']               \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_166 (Batch  (None, 60, 304)     1216        ['permute_56[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_43 (Add)                   (None, 304, 60)      0           ['batch_normalization_163[0][0]',\n",
            "                                                                  'multi_head_attention_9[1][0]'] \n",
            "                                                                                                  \n",
            " tf_op_layer_Shape_18 (TensorFl  [(3,)]              0           ['batch_normalization_166[0][0]']\n",
            " owOpLayer)                                                                                       \n",
            "                                                                                                  \n",
            " bidirectional_37 (Bidirectiona  (None, 304, 60)     58080       ['add_43[0][0]']                 \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " tf_op_layer_strided_slice_18 (  [()]                0           ['tf_op_layer_Shape_18[0][0]']   \n",
            " TensorFlowOpLayer)                                                                               \n",
            "                                                                                                  \n",
            " activation_83 (Activation)     (None, 304, 60)      0           ['bidirectional_37[0][0]']       \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_18/multiples   [(3,)]              0           ['tf_op_layer_strided_slice_18[0]\n",
            " (TensorFlowOpLayer)                                             [0]']                            \n",
            "                                                                                                  \n",
            " batch_normalization_165 (Batch  (None, 304, 60)     240         ['activation_83[0][0]']          \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_18 (TensorFlo  [(None, 304, 304)]  0           ['tf_op_layer_Tile_18/multiples[0\n",
            " wOpLayer)                                                       ][0]']                           \n",
            "                                                                                                  \n",
            " permute_57 (Permute)           (None, 60, 304)      0           ['batch_normalization_165[0][0]']\n",
            "                                                                                                  \n",
            " tf_op_layer_MatMul_32 (TensorF  [(None, 60, 304)]   0           ['batch_normalization_166[0][0]',\n",
            " lowOpLayer)                                                      'tf_op_layer_Tile_18[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_167 (Batch  (None, 60, 304)     1216        ['permute_57[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " tf_op_layer_MatMul_33 (TensorF  [(None, 60, 60)]    0           ['tf_op_layer_MatMul_32[0][0]',  \n",
            " lowOpLayer)                                                      'batch_normalization_167[0][0]']\n",
            "                                                                                                  \n",
            " tf_op_layer_Tanh_8 (TensorFlow  [(None, 60, 60)]    0           ['tf_op_layer_MatMul_33[0][0]']  \n",
            " OpLayer)                                                                                         \n",
            "                                                                                                  \n",
            " tf_op_layer_Max_16 (TensorFlow  [(None, 60, 1)]     0           ['tf_op_layer_Tanh_8[0][0]']     \n",
            " OpLayer)                                                                                         \n",
            "                                                                                                  \n",
            " tf_op_layer_Max_17 (TensorFlow  [(None, 1, 60)]     0           ['tf_op_layer_Tanh_8[0][0]']     \n",
            " OpLayer)                                                                                         \n",
            "                                                                                                  \n",
            " tf_op_layer_transpose_8 (Tenso  [(None, 304, 60)]   0           ['batch_normalization_166[0][0]']\n",
            " rFlowOpLayer)                                                                                    \n",
            "                                                                                                  \n",
            " tf_op_layer_Softmax_16 (Tensor  [(None, 60, 1)]     0           ['tf_op_layer_Max_16[0][0]']     \n",
            " FlowOpLayer)                                                                                     \n",
            "                                                                                                  \n",
            " tf_op_layer_Softmax_17 (Tensor  [(None, 1, 60)]     0           ['tf_op_layer_Max_17[0][0]']     \n",
            " FlowOpLayer)                                                                                     \n",
            "                                                                                                  \n",
            " tf_op_layer_MatMul_34 (TensorF  [(None, 304, 1)]    0           ['tf_op_layer_transpose_8[0][0]',\n",
            " lowOpLayer)                                                      'tf_op_layer_Softmax_16[0][0]'] \n",
            "                                                                                                  \n",
            " tf_op_layer_MatMul_35 (TensorF  [(None, 1, 304)]    0           ['tf_op_layer_Softmax_17[0][0]', \n",
            " lowOpLayer)                                                      'batch_normalization_167[0][0]']\n",
            "                                                                                                  \n",
            " tf_op_layer_Squeeze_16 (Tensor  [(None, 304)]       0           ['tf_op_layer_MatMul_34[0][0]']  \n",
            " FlowOpLayer)                                                                                     \n",
            "                                                                                                  \n",
            " tf_op_layer_Squeeze_17 (Tensor  [(None, 304)]       0           ['tf_op_layer_MatMul_35[0][0]']  \n",
            " FlowOpLayer)                                                                                     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15,950,620\n",
            "Trainable params: 15,945,900\n",
            "Non-trainable params: 4,720\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max Pooling (with Tests)"
      ],
      "metadata": {
        "id": "1s_ldDYnncaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maxp_ia_model(input_nodes, N, pfx=''):\n",
        "    qi_feat, si_feat = ap_interactive_attention_model(N=N, pfx='sa_q'+pfx, input_nodes=input_nodes)\n",
        "    # apply column wise max pooling (correct)\n",
        "    maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))\n",
        "    # apply row wise max pooling ??\n",
        "    # maxpool = Lambda(lambda x: K.max(x, axis=2, keepdims=False), output_shape=lambda x: (x[0], x[1]))\n",
        "\n",
        "    maxpool.supports_masking = True\n",
        "    question_pool = maxpool(qi_feat)\n",
        "    answer_pool = maxpool(si_feat)\n",
        "    print(\"question_pool.shape\", question_pool.shape)\n",
        "    print(\"answer_pool.shape\", answer_pool.shape)\n",
        "\n",
        "    return [question_pool, answer_pool]"
      ],
      "metadata": {
        "id": "T26uw9InncaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N, input_nodes_emb, output_nodes_emb = embedding()\n",
        "conf['proj'] = False\n",
        "iam = maxp_ia_model(output_nodes_emb, N, pfx= 'S')\n",
        "test_model = Model(inputs=input_nodes_emb, outputs=iam)\n",
        "test_model.summary()\n",
        "test_model.compile(loss=ranknet, optimizer=conf['opt'])"
      ],
      "metadata": {
        "id": "T0NtmFS1ncaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avg Pooling (with Tests)"
      ],
      "metadata": {
        "id": "Duwjh7Nzncaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def avgp_ia_model(input_nodes, N, pfx=''):\n",
        "    qi_feat, si_feat = ap_interactive_attention_model(N=N, pfx='sa_q'+pfx, input_nodes=input_nodes)\n",
        "    # apply column wise mean pooling (correct)\n",
        "    meanpool = Lambda(lambda x: K.mean(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))\n",
        "    # apply row wise mean pooling ??\n",
        "    # meanpool = Lambda(lambda x: K.mean(x, axis=2, keepdims=False), output_shape=lambda x: (x[0], x[1]))\n",
        "\n",
        "    meanpool.supports_masking = True\n",
        "    question_pool = meanpool(qi_feat)\n",
        "    answer_pool = meanpool(si_feat)\n",
        "    print(\"question_pool.shape\", question_pool.shape)\n",
        "    print(\"answer_pool.shape\", answer_pool.shape)\n",
        "\n",
        "    return [question_pool, answer_pool]"
      ],
      "metadata": {
        "id": "BD0veTaZncaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N, input_nodes_emb, output_nodes_emb = embedding()\n",
        "conf['proj'] = False\n",
        "iam = avgp_ia_model(output_nodes_emb, N, pfx= 'S')\n",
        "test_model = Model(inputs=input_nodes_emb, outputs=iam)\n",
        "test_model.summary()\n",
        "test_model.compile(loss=ranknet, optimizer=conf['opt'])"
      ],
      "metadata": {
        "id": "Zy5yEe75ncaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrgMEX9ob8iz"
      },
      "source": [
        "## MLP scoring function\n",
        "To compare two sentence vectors, we used the mlp similarity function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uau62_Ldb8i0"
      },
      "outputs": [],
      "source": [
        "def mlp_ptscorer(inputs, Ddim, N, l2reg, pfx='out', oact='sigmoid', extra_inp=[]):\n",
        "    \"\"\" Element-wise features from the pair fed to an MLP. \"\"\"\n",
        "\n",
        "    sum_vec = add(inputs)\n",
        "    mul_vec = multiply(inputs)\n",
        "\n",
        "    mlp_input = concatenate([sum_vec, mul_vec])\n",
        "\n",
        "    # Ddim may be either 0 (no hidden layer), scalar (single hidden layer) or\n",
        "    # list (multiple hidden layers)\n",
        "    if Ddim == 0:\n",
        "        Ddim = []\n",
        "    elif not isinstance(Ddim, list):\n",
        "        Ddim = [Ddim]\n",
        "    if Ddim:\n",
        "        for i, D in enumerate(Ddim):\n",
        "            shared_dense = Dense(int(N*D), kernel_regularizer=l2(l2reg), \n",
        "                                 activation='linear', name=pfx+'hdn%d'%(i))\n",
        "            mlp_input = Activation('tanh')(shared_dense(mlp_input))\n",
        "\n",
        "    shared_dense = Dense(1, kernel_regularizer=l2(l2reg), activation=oact, name=pfx+'mlp')\n",
        "    mlp_out = shared_dense(mlp_input)\n",
        "    \n",
        "    return mlp_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czwkarimb8i1"
      },
      "source": [
        "## Model Architecture "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V1U5BBeub8i1"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    # input embedding         \n",
        "    N, input_nodes_emb, output_nodes_emb = embedding() \n",
        "    # Projection layer = false for attentive pooling \n",
        "    conf['proj'] = False\n",
        "    # answer sentence selection\n",
        "    # avg_model / rnn_model / cnn_model / \n",
        "    # self_attention_model / ap_model / maxp_model / avgp_model\n",
        "    # interactive_attention_model / ap_ia_model / maxp_ia_model / avgp_ia_model\n",
        "\n",
        "    ptscorer_inputs = interactive_attention_model(output_nodes_emb, N, pfx='S')\n",
        "    print(\"Using interactive_attention_model model...\")\n",
        "\n",
        "    scoreS = mlp_ptscorer(ptscorer_inputs, conf['Ddim'], N,  \n",
        "            conf['l2reg'], pfx='outS', oact='sigmoid')                \n",
        "\n",
        "    output_nodes = scoreS\n",
        "\n",
        "    model = Model(inputs=input_nodes_emb, outputs=output_nodes)\n",
        "    \n",
        "    model.compile(loss=ranknet, optimizer=conf['opt'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cusfEfH3b8i1"
      },
      "source": [
        "## Train and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ov1mCxCdb8i2"
      },
      "outputs": [],
      "source": [
        "def train_and_eval(runid):\n",
        "    print('Model')\n",
        "    model = build_model()\n",
        "    print(model.summary())\n",
        "    \n",
        "    print('Training')\n",
        "    fit_model(model, weightsf='weights-'+runid+'-bestval.h5py')\n",
        "    model.save_weights('weights-'+runid+'-final.h5py', overwrite=True)\n",
        "    model.load_weights('weights-'+runid+'-bestval.h5py')\n",
        "\n",
        "    print('Predict&Eval (best val epoch)')\n",
        "    res = eval(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Znp5kB6ub8i2"
      },
      "outputs": [],
      "source": [
        "def fit_model(model, **kwargs):\n",
        "    epochs = conf['epochs']\n",
        "    callbacks = fit_callbacks(kwargs.pop('weightsf'))\n",
        "    \n",
        "    return model.fit(inp_tr, y=y_train, validation_data=[inp_val, y_val], batch_size=conf['batch_size'],\n",
        "                     callbacks = callbacks, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO8Npu78b8i3"
      },
      "source": [
        "\n",
        "At every epoch, the callback function measures mrr performance and accuracy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8XmAIK6Pb8i3"
      },
      "outputs": [],
      "source": [
        "def fit_callbacks(weightsf):                                  \n",
        "    return [AnsSelCB(inp_val['q'], inp_val['sents'], y_val, inp_val),\n",
        "            ModelCheckpoint(weightsf, save_best_only=True, monitor='mrr', mode='max'),\n",
        "            EarlyStopping(monitor='mrr', mode='max', patience=conf['patience'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wciKiH5sb8i4"
      },
      "outputs": [],
      "source": [
        "def eval(model):\n",
        "    res = []\n",
        "    for inp in [inp_val, inp_test]:\n",
        "        if inp is None:\n",
        "            res.append(None)\n",
        "            continue\n",
        "\n",
        "        pred = model.predict(inp)\n",
        "        res.append(eval_QA(pred, inp['q'], inp['y']))\n",
        "    pred = model.predict(inp_test)\n",
        "    string1 = \"When was Florence Nightingale born ?\"\n",
        "    sorted_output(inp_test['q'], inp_test['sents'], inp_test['y'], pred, 1, string1)\n",
        "    return tuple(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Code"
      ],
      "metadata": {
        "id": "rr2O7i8vZ7Oo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W63mXZrBb8i4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "647415fa-f476-4a61-a15e-b142db985d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe\n",
            "Dataset\n",
            "Vocabulary of 51694 words\n",
            "RunID: Model--2747bc947d8848f7  ({\"Ddim\": \"2\", \"adim\": \"0.5\", \"batch_size\": \"320\", \"bll_dropout\": \"0.5\", \"cfiltlen\": \"3\", \"cnn_dropout\": \"0.5\", \"cnnact\": \"relu\", \"cnninit\": \"glorot_uniform\", \"emb\": \"Glove\", \"embdim\": \"300\", \"epochs\": \"5\", \"flag\": \"True\", \"inp_e_dropout\": \"0.5\", \"l2reg\": \"0.0001\", \"opt\": \"adam\", \"p_dropout\": \"0.5\", \"p_init\": \"glorot_uniform\", \"p_layers\": \"1\", \"pact\": \"tanh\", \"pad\": \"60\", \"patience\": \"155\", \"pdim\": \"0.5\", \"pe\": \"True\", \"pe_method\": \"fixed\", \"pool_layer\": \"<class 'keras.layers.pooling.MaxPooling1D'>\", \"proj\": \"True\", \"rnn\": \"<class 'keras.layers.cudnn_recurrent.CuDNNLSTM'>\", \"rnn_dropout\": \"0.5\", \"rnnact\": \"tanh\", \"rnnbidi\": \"True\", \"rnnbidi_mode\": \"<function concatenate at 0x7f6d585954d0>\", \"rnninit\": \"glorot_uniform\", \"sdim\": \"5\", \"self_pdim\": \"0.5\", \"w_feat_model\": \"rnn\"})\n",
            "Model\n",
            "(?, 60, 60)\n",
            "(?, 60, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 180)\n",
            "(?, 304, 180)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "(?, 304, 60)\n",
            "qi_val.shape:  (?, 304)\n",
            "si_val.shape:  (?, 304)\n",
            "Using interactive_attention_model model...\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " qi (InputLayer)                [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " emb (Embedding)                (None, 60, 300)      15508200    ['qi[0][0]',                     \n",
            "                                                                  'si[0][0]']                     \n",
            "                                                                                                  \n",
            " f01 (InputLayer)               [(None, 60, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenate)   (None, 60, 304)      0           ['emb[0][0]',                    \n",
            "                                                                  'f01[0][0]']                    \n",
            "                                                                                                  \n",
            " si (InputLayer)                [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " tf_op_layer_Shape_10 (TensorFl  [(3,)]              0           ['concatenate_18[0][0]']         \n",
            " owOpLayer)                                                                                       \n",
            "                                                                                                  \n",
            " f10 (InputLayer)               [(None, 60, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " tf_op_layer_strided_slice_10 (  [()]                0           ['tf_op_layer_Shape_10[0][0]']   \n",
            " TensorFlowOpLayer)                                                                               \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 60, 304)      0           ['emb[1][0]',                    \n",
            "                                                                  'f10[0][0]']                    \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_10/multiples   [(3,)]              0           ['tf_op_layer_strided_slice_10[0]\n",
            " (TensorFlowOpLayer)                                             [0]']                            \n",
            "                                                                                                  \n",
            " tf_op_layer_Shape_11 (TensorFl  [(3,)]              0           ['concatenate_19[0][0]']         \n",
            " owOpLayer)                                                                                       \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_10 (TensorFlo  [(None, 60, 304)]   0           ['tf_op_layer_Tile_10/multiples[0\n",
            " wOpLayer)                                                       ][0]']                           \n",
            "                                                                                                  \n",
            " tf_op_layer_strided_slice_11 (  [()]                0           ['tf_op_layer_Shape_11[0][0]']   \n",
            " TensorFlowOpLayer)                                                                               \n",
            "                                                                                                  \n",
            " multiply_14 (Multiply)         (None, 60, 304)      0           ['concatenate_18[0][0]',         \n",
            "                                                                  'tf_op_layer_Tile_10[0][0]']    \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_11/multiples   [(3,)]              0           ['tf_op_layer_strided_slice_11[0]\n",
            " (TensorFlowOpLayer)                                             [0]']                            \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 60, 304)      0           ['multiply_14[0][0]']            \n",
            "                                                                                                  \n",
            " tf_op_layer_Tile_11 (TensorFlo  [(None, 60, 304)]   0           ['tf_op_layer_Tile_11/multiples[0\n",
            " wOpLayer)                                                       ][0]']                           \n",
            "                                                                                                  \n",
            " time_distributed_26 (TimeDistr  (None, 60, 304)     92720       ['dropout_15[0][0]']             \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " multiply_15 (Multiply)         (None, 60, 304)      0           ['concatenate_19[0][0]',         \n",
            "                                                                  'tf_op_layer_Tile_11[0][0]']    \n",
            "                                                                                                  \n",
            " time_distributed_28 (TimeDistr  (None, 60, 304)     1216        ['time_distributed_26[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 60, 304)      0           ['multiply_15[0][0]']            \n",
            "                                                                                                  \n",
            " time_distributed_30 (TimeDistr  (None, 60, 304)     0           ['time_distributed_28[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " time_distributed_27 (TimeDistr  (None, 60, 304)     92720       ['dropout_16[0][0]']             \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " permute_20 (Permute)           (None, 304, 60)      0           ['time_distributed_30[0][0]']    \n",
            "                                                                                                  \n",
            " time_distributed_29 (TimeDistr  (None, 60, 304)     1216        ['time_distributed_27[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " batch_normalization_63 (BatchN  (None, 304, 60)     240         ['permute_20[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " time_distributed_31 (TimeDistr  (None, 60, 304)     0           ['time_distributed_29[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " permute_21 (Permute)           (None, 304, 60)      0           ['time_distributed_31[0][0]']    \n",
            "                                                                                                  \n",
            " permute_22 (Permute)           (None, 60, 304)      0           ['batch_normalization_63[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_64 (BatchN  (None, 304, 60)     240         ['permute_21[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_65 (BatchN  (None, 60, 304)     1216        ['permute_22[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dot_15 (Dot)                   (None, 60, 60)       0           ['batch_normalization_65[0][0]', \n",
            "                                                                  'batch_normalization_64[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_66 (BatchN  (None, 60, 60)      240         ['dot_15[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " permute_23 (Permute)           (None, 60, 60)       0           ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_67 (BatchN  (None, 60, 60)      240         ['permute_23[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 60, 60)       0           ['batch_normalization_67[0][0]'] \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 60, 60)       0           ['batch_normalization_66[0][0]'] \n",
            "                                                                                                  \n",
            " dot_17 (Dot)                   (None, 304, 60)      0           ['batch_normalization_64[0][0]', \n",
            "                                                                  'activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " dot_16 (Dot)                   (None, 304, 60)      0           ['batch_normalization_63[0][0]', \n",
            "                                                                  'activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_69 (BatchN  (None, 304, 60)     240         ['dot_17[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_68 (BatchN  (None, 304, 60)     240         ['dot_16[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dot_18 (Dot)                   (None, 304, 60)      0           ['batch_normalization_68[0][0]', \n",
            "                                                                  'activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " dot_19 (Dot)                   (None, 304, 60)      0           ['batch_normalization_69[0][0]', \n",
            "                                                                  'activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_70 (BatchN  (None, 304, 60)     240         ['dot_18[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_71 (BatchN  (None, 304, 60)     240         ['dot_19[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenate)   (None, 304, 180)     0           ['batch_normalization_63[0][0]', \n",
            "                                                                  'batch_normalization_69[0][0]', \n",
            "                                                                  'batch_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 304, 180)     0           ['batch_normalization_64[0][0]', \n",
            "                                                                  'batch_normalization_68[0][0]', \n",
            "                                                                  'batch_normalization_71[0][0]'] \n",
            "                                                                                                  \n",
            " bidirectional_12 (Bidirectiona  (None, 304, 60)     50640       ['concatenate_20[0][0]']         \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " bidirectional_13 (Bidirectiona  (None, 304, 60)     50640       ['concatenate_21[0][0]']         \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 304, 60)      0           ['bidirectional_12[0][0]']       \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 304, 60)      0           ['bidirectional_13[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_72 (BatchN  (None, 304, 60)     240         ['activation_36[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_73 (BatchN  (None, 304, 60)     240         ['activation_37[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  ((None, 304, 60),   97260       ['batch_normalization_72[0][0]', \n",
            " eadAttention)                   (None, 4, 304, 304               'batch_normalization_72[0][0]', \n",
            "                                ))                                'batch_normalization_73[0][0]', \n",
            "                                                                  'batch_normalization_73[0][0]'] \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 304, 60)      3660        ['multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 304, 60)      3660        ['multi_head_attention_3[1][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_74 (BatchN  (None, 304, 60)     240         ['dense_6[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_75 (BatchN  (None, 304, 60)     240         ['dense_7[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 304, 60)      0           ['batch_normalization_74[0][0]', \n",
            "                                                                  'multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 304, 60)      0           ['batch_normalization_75[0][0]', \n",
            "                                                                  'multi_head_attention_3[1][0]'] \n",
            "                                                                                                  \n",
            " bidirectional_14 (Bidirectiona  (None, 304, 60)     58080       ['add_8[0][0]']                  \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " bidirectional_15 (Bidirectiona  (None, 304, 60)     58080       ['add_9[0][0]']                  \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 304, 60)      0           ['bidirectional_14[0][0]']       \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 304, 60)      0           ['bidirectional_15[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_76 (BatchN  (None, 304, 60)     240         ['activation_38[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_77 (BatchN  (None, 304, 60)     240         ['activation_39[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " permute_24 (Permute)           (None, 60, 304)      0           ['batch_normalization_76[0][0]'] \n",
            "                                                                                                  \n",
            " permute_25 (Permute)           (None, 60, 304)      0           ['batch_normalization_77[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_78 (BatchN  (None, 60, 304)     1216        ['permute_24[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_79 (BatchN  (None, 60, 304)     1216        ['permute_25[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " bowS (Lambda)                  (None, 304)          0           ['batch_normalization_78[0][0]', \n",
            "                                                                  'batch_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 304)          0           ['bowS[0][0]',                   \n",
            "                                                                  'bowS[1][0]']                   \n",
            "                                                                                                  \n",
            " multiply_16 (Multiply)         (None, 304)          0           ['bowS[0][0]',                   \n",
            "                                                                  'bowS[1][0]']                   \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, 608)          0           ['add_10[0][0]',                 \n",
            "                                                                  'multiply_16[0][0]']            \n",
            "                                                                                                  \n",
            " outShdn0 (Dense)               (None, 608)          370272      ['concatenate_22[0][0]']         \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 608)          0           ['outShdn0[0][0]']               \n",
            "                                                                                                  \n",
            " outSmlp (Dense)                (None, 1)            609         ['activation_40[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16,303,261\n",
            "Trainable params: 16,298,541\n",
            "Non-trainable params: 4,720\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Training\n",
            "Train on 44647 samples, validate on 1148 samples\n",
            "Epoch 1/5\n",
            "22720/44647 [==============>...............] - ETA: 10:13 - loss: 0.7481"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-046466106c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Final Model Training and Evaluation (Uncomment when module check is complete)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-2451af5d9abb>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(runid)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightsf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weights-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrunid\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-bestval.h5py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrunid\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-final.h5py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrunid\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-bestval.h5py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-411d3b0c6248>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     return model.fit(inp_tr, y=y_train, validation_data=[inp_val, y_val], batch_size=conf['batch_size'],\n\u001b[0;32m----> 6\u001b[0;31m                      callbacks = callbacks, epochs=epochs)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4275\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 4276\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   4277\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4278\u001b[0m     output_structure = tf.nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1481\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    path = '/content/gdrive/MyDrive/FYP_Project/data'\n",
        "    trainf = path + '/train-all.csv' \n",
        "    valf = path + '/dev.csv'\n",
        "    testf = path + '/test.csv'\n",
        "    params = []\n",
        "    \n",
        "    conf, ps, h = config()\n",
        "\n",
        "    # Uncomment for GloVe embedding\n",
        "    if conf['emb'] == 'Glove': # Please download the GloVe in here http://nlp.stanford.edu/data/glove.6B.zip\n",
        "        print('GloVe')\n",
        "        emb = GloVe(N=conf['embdim'])\n",
        "\n",
        "    print('Dataset')\n",
        "    load_data(trainf,valf,testf)\n",
        "    runid = 'Model-%x' % (h)\n",
        "    print('RunID: %s  (%s)' % (runid, ps))\n",
        "    \n",
        "    # Final Model Training and Evaluation (Uncomment when module check is complete)\n",
        "    train_and_eval(runid)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict for question using Saved Model"
      ],
      "metadata": {
        "id": "TOgmDWq7GRAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/gdrive/MyDrive/FYP_Project/data'\n",
        "trainf = path + '/train-all.csv' \n",
        "valf = path + '/dev.csv'\n",
        "testf = path + '/test.csv'\n",
        "params = []\n",
        "\n",
        "conf, ps, h = config()\n",
        "\n",
        "# Uncomment for GloVe embedding\n",
        "if conf['emb'] == 'Glove': # Please download the GloVe in here http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    print('GloVe')\n",
        "    emb = GloVe(N=conf['embdim'])\n",
        "\n",
        "print('Dataset')\n",
        "load_data(trainf,valf,testf)\n",
        "runid = 'Model-%x' % (h)\n",
        "print('RunID: %s  (%s)' % (runid, ps))\n",
        "print('Model')\n",
        "model = build_model()\n",
        "print(model.summary())\n",
        "\n",
        "model.load_weights('weights-'+runid+'-bestval.h5py')\n",
        "res = []\n",
        "\n",
        "pred = model.predict(inp_test)\n",
        "res.append(eval_QA(pred, inp_test['q'], inp_test['y']))\n",
        "string1 = \"When was Florence Nightingale born ?\"\n",
        "sorted_output(inp_test['q'], inp_test['sents'], inp_test['y'], pred, 1, string1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oTnbH-GGVC3",
        "outputId": "12d436cd-8b05-45ae-c76d-d284884d4474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe\n",
            "Dataset\n",
            "Vocabulary of 51694 words\n",
            "RunID: Model--4b958568ebe118da  ({\"Ddim\": \"2\", \"adim\": \"0.5\", \"batch_size\": \"320\", \"bll_dropout\": \"0.5\", \"cfiltlen\": \"3\", \"cnn_dropout\": \"0.5\", \"cnnact\": \"relu\", \"cnninit\": \"glorot_uniform\", \"emb\": \"Glove\", \"embdim\": \"300\", \"epochs\": \"5\", \"flag\": \"True\", \"inp_e_dropout\": \"0.5\", \"l2reg\": \"0.0001\", \"opt\": \"adam\", \"p_dropout\": \"0.5\", \"p_init\": \"glorot_uniform\", \"p_layers\": \"1\", \"pact\": \"tanh\", \"pad\": \"60\", \"patience\": \"155\", \"pdim\": \"0.5\", \"pe\": \"True\", \"pe_method\": \"learned\", \"pool_layer\": \"<class 'keras.layers.pooling.MaxPooling1D'>\", \"proj\": \"True\", \"rnn\": \"<class 'keras.layers.cudnn_recurrent.CuDNNLSTM'>\", \"rnn_dropout\": \"0.5\", \"rnnact\": \"tanh\", \"rnnbidi\": \"True\", \"rnnbidi_mode\": \"<function concatenate at 0x7fa121034290>\", \"rnninit\": \"glorot_uniform\", \"sdim\": \"5\", \"self_pdim\": \"0.5\", \"w_feat_model\": \"rnn\"})\n",
            "Model\n",
            "Using maxp_ia_model model...\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " qi (InputLayer)                [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " si (InputLayer)                [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " emb (Embedding)                (None, 60, 300)      15508200    ['qi[0][0]',                     \n",
            "                                                                  'si[0][0]']                     \n",
            "                                                                                                  \n",
            " f01 (InputLayer)               [(None, 60, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " pos_input (InputLayer)         [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " f10 (InputLayer)               [(None, 60, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 60, 304)      0           ['emb[0][0]',                    \n",
            "                                                                  'f01[0][0]']                    \n",
            "                                                                                                  \n",
            " pe_learnable_layer (Embedding)  (None, 60, 304)     18240       ['pos_input[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 60, 304)      0           ['emb[1][0]',                    \n",
            "                                                                  'f10[0][0]']                    \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 60, 304)      0           ['concatenate_3[0][0]',          \n",
            "                                                                  'pe_learnable_layer[0][0]']     \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 60, 304)      0           ['concatenate_4[0][0]',          \n",
            "                                                                  'pe_learnable_layer[0][0]']     \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 60, 304)      0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 60, 304)      0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " time_distributed_6 (TimeDistri  (None, 60, 304)     92720       ['dropout_2[0][0]']              \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            " time_distributed_7 (TimeDistri  (None, 60, 304)     92720       ['dropout_3[0][0]']              \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            " time_distributed_8 (TimeDistri  (None, 60, 304)     1216        ['time_distributed_6[0][0]']     \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            " time_distributed_9 (TimeDistri  (None, 60, 304)     1216        ['time_distributed_7[0][0]']     \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            " time_distributed_10 (TimeDistr  (None, 60, 304)     0           ['time_distributed_8[0][0]']     \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " time_distributed_11 (TimeDistr  (None, 60, 304)     0           ['time_distributed_9[0][0]']     \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " bowS (Lambda)                  (None, 304)          0           ['time_distributed_10[0][0]',    \n",
            "                                                                  'time_distributed_11[0][0]']    \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 304)          0           ['bowS[0][0]',                   \n",
            "                                                                  'bowS[1][0]']                   \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 304)          0           ['bowS[0][0]',                   \n",
            "                                                                  'bowS[1][0]']                   \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 608)          0           ['add_5[0][0]',                  \n",
            "                                                                  'multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " outShdn0 (Dense)               (None, 608)          370272      ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 608)          0           ['outShdn0[0][0]']               \n",
            "                                                                                                  \n",
            " outSmlp (Dense)                (None, 1)            609         ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15,992,473\n",
            "Trainable params: 15,991,257\n",
            "Non-trainable params: 1,216\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MRR: 0.621210\n",
            "MAP: 0.604001\n",
            "Question:\n",
            "What do practitioners of Wicca worship ?\n",
            "\n",
            "Candidate answers sorted by ypred score:\n",
            "That 's because Ms . Palmer is a witch , the high priestess of a group that practices Wicca at Fort Hood with the knowledge and approval of the U.S . Army .\n",
            "Wicca members tend to be white , college-educated , middle-class women , many of whom have families , said Ms . Berger , who said the religion was becoming more institutionalized as the members aged .\n",
            "Ms . Siefferly , a senior at the high school , has practiced Wicca for several years after reading a book on the subject , Schram said .\n",
            "Such sentiments are rooted in fear and false notions that witches worship Satan or sacrifice animals , said David Oringderff , head of the San Antonio -based pagan group that sponsors Ms . Palmer 's Fort Hood Open Circle .\n",
            "The inch- thick chaplain handbook includes a five -page primer on Wicca , described as `` a reconstruction of the Nature worship of tribal Europe . ''\n",
            "The fear of a backlash has never disappeared , said Ms . Berger , who added that the false stereotype that witches worship Satan endures .\n",
            "`` When people think of Wicca , they think of either Satanism or silly mumbo jumbo. ''\n",
            "Wicca -- sometimes spelled Wycca -- comes from the Old English word for witch .\n",
            "An estimated <num> Americans practice Wicca , a form of polytheistic nature worship .\n",
            "`` If they deny our right to worship ...\n",
            "Question:\n",
            "Who is the president or chief executive of Amtrak ?\n",
            "\n",
            "Candidate answers sorted by ypred score:\n",
            "`` Today we are kicking off the revitalization of train service in America , '' Gov . Tommy G . Thompson of Wisconsin , who is the chairman of Amtrak 's board , said at a Manhattan news conference .\n",
            "But more important for Amtrak , the high-speed train provides a model for its rail service across the nation , and is the centerpiece of its hopes of economic redemption after operating for three decades with government subsidies .\n",
            "NEW YORK _ Aiming to bring high-speed rail service to Boston by the start of the new millennium , Amtrak said Tuesday that it would start rolling out the first of its new <num> mile -an-hour trains by December .\n",
            "For the second time in nine months , Amtrak has settled a racial discrimination suit , the latest alleging the national railway condoned hostile work conditions for blacks that created a '' segregated work force . ''\n",
            "Amtrak , which must wean itself from federal operating subsidies by <num> , hopes its planned service improvements -- such as flowers in bathrooms and entertainment for children -- will boost revenues by $ <num> million over three years .\n",
            "Warrington and the president of Bombardier , Jacques Lapare , were not specific about the cause of the problem , but Lapare said that it became apparent only because of instrumentation on a test train at Pueblo .\n",
            "The suit was filed by <num> black members of the Brotherhood of Maintenance of Way Employees who build and maintain railroad tracks for Amtrak 's engineering department in the Northeast corridor between Washington and Boston .\n",
            "In a conference call with reporters to discuss the company 's plans , Warrington said that before Congress created Amtrak a quarter-century ago , nearly half of railroads ' passenger -train revenue actually came from hauling freight .\n",
            "`` They 've already added so much time to the schedule of the Chief , '' said Ross Capon , executive director of the National Association of Railroad Passengers , `` and already had so many horror stories. ''\n",
            "George Warrington , Amtrak 's president , said ridership was up because service was better and because `` the marketplace is feeling increasingly frustrated with the alternatives , '' namely jammed highways and delayed airplanes .\n",
            "`` Amtrak has a zero -tolerance policy for discriminatory practices in hiring or in any aspect of the workplace , '' said George D . Warrington , Amtrak 's acting president and chief executive officer .\n",
            "`` Long-term success here has to do with doing it right , getting it right and increasing market share , '' said George Warrington , Amtrak 's president and chief executive .\n",
            "George Warrington , Amtrak 's president and chief executive , said the railroad would face a shortfall in the next fiscal year as a result of the delay .\n",
            "With new , electrified engines pulling rebuilt Amtrak cars , the three-hour schedule has been made possible by extensive track work and electrification from New Haven to Boston , a project that cost $ <num> billion .\n",
            "Amtrak will lose money again this year , but will meet the congressional deadline of weaning itself from operating subsidies by the fiscal year ending Sept . <num> , <num> , officials said .\n",
            "Another question left unanswered Tuesday was how many stops the high-speed trains would make , another complex equation for Amtrak planners , since every stop increases travel time for those on board .\n",
            "`` We will treat our customers as valued guests ... , so they will choose to ride our trains over and over again , '' said George Warrington , Amtrak 's president and chief executive .\n",
            "Sen . John McCain panned a plan that would give Amtrak a '' multibillion-dollar blank check '' even though the railway is under orders to become independent from federal operating subsidies .\n",
            "In February , a subcommittee led by Amtrak supporter Sen . Kay Bailey Hutchison , R - Texas , endorsed the railway 's plan to expand rather than cut service as the route to fiscal health .\n",
            "'' Amtrak is largely no better off today than one year ago , '' said Phyllis Scheinberg , associate director for transportation issues at the General Accounting Office .\n",
            "The white female plaintiff , Leslie Dunning of Fairfax Station , Va . , said Amtrak demoted her in retaliation for helping a black manager prepare a report that detailed discrimination within the company .\n",
            "The lawsuit , filed last August in the U.S . District Court for the District of Columbia , contended Amtrak discriminated against blacks in filling management jobs and handling pay , promotion and disciplinary issues .\n",
            "But Hutchison and pro- Amtrak Democrats Max Cleland of Georgia and John Kerry of Massachusetts had to settle for supporting roles Tuesday as McCain offered an unflattering history lesson about Amtrak , which runs two routes that cross Arizona .\n",
            "Amtrak is counting on the new service , called Acela , to lure business away from the airlines and to generate $ <num> million a year in profits .\n",
            "To attract more riders , Amtrak said it would schedule more trains along the length of its Northeast corridor , with the biggest service expansion between Boston and New York City .\n",
            "Amtrak is now in its 29th year of subsidies and has consumed more than $ <num> billion in federal operating support .\n",
            "'' There 's a lot that Amtrak has agreed to here that would not have happened if this had gone into litigation , '' she said .\n",
            "Amtrak also said Wednesday that it would close its food warehouses and hire Dobbs International Services , which already provides food to airlines , to do the same with trains .\n",
            "`` We are going to guarantee passenger rights while others in the transportation industry are fighting them in Congress , '' said George Warrington , Amtrak 's president .\n",
            "Amtrak President George Warrington told the committee that the railway expects delivery next week of the first of <num> high-speed trains for the Boston -to- Washington Northeast Corridor .\n",
            "Thompson , Mead and Scheinberg agreed that , even if Amtrak weans itself from federal operating subsidies by <num> , it will need significant continuing federal support for capital expenses like track repair .\n",
            "Amtrak advocates say the money represents an investment in intercity rail that should be considered independent of Amtrak 's future as a corporation .\n",
            "It would let Amtrak issue bonds , sweetened by federal tax credits , to raise $ <num> billion over <num> years to build high-speed rail lines around the country .\n",
            "Amtrak passenger revenues exceeded $ <num> billion for the first time ever last year , with <num> million customers riding by rail .\n",
            "Moscow is insisting on a separate sector for its peacekeepers ; NATO says that wo n't happen though President Clinton predicted a `` successful conclusion. ''\n",
            "Amtrak , wooing people to take the train , not the plane , today promised all passengers good service or a voucher for a free ride .\n",
            "Michael Lieder , lead counsel for the plaintiffs , said that the new practices `` will help employees of all races survive and thrive at Amtrak well into the future. ''\n",
            "The earlier lawsuit , which included two Amtrak workers from Massachusetts , opened the door for the second suit , lawyers said .\n",
            "Both Scheinberg and Kenneth Mead , inspector general in the Department of Transportation , said Amtrak is not on track to meet congressional demands that it achieve financial self-sufficiency by <num> .\n",
            "Black track-laying and repair workers in the Northeast filed a class-action suit in April , also charging unfair treatment of blacks and likening Amtrak to a `` plantation . ''\n",
            "Amtrak is also upgrading the tracks between Washington and Boston , said Warrington , which should lead to improved service even before the high-speed trains are introduced .\n",
            "'' I am now embarrassed to admit that I worked to create Amtrak , '' Joseph Vranich , a former Amtrak spokesman turned critic , told the committee .\n",
            "In addition , Amtrak has created a new position , vice president of business diversity and strategic initiatives , and filled the job .\n",
            "Tuesday 's hearing was a dramatic departure from the last time Amtrak 's fate was considered by Commerce Committee members .\n",
            "Amtrak is offering vouchers , instead of refunds on the current trip , to give itself a second chance to woo the customer .\n",
            "'' For much of Amtrak 's history , it was not run like a business , '' Thompson said .\n",
            "Amtrak did not admit any wrongdoing in the settlement announced on Friday , but the railway agreed to change company policies .\n",
            "Central to winning customers ' confidence is Amtrak 's new pledge : Amtrak will offer dissatisfied passengers a refund voucher good for future travel .\n",
            "Under terms of the settlement , Amtrak did not admit to any wrongdoing , but agreed to extensive changes in corporate practices designed to prevent race discrimination .\n",
            "Mead forecasts that Amtrak will lose $ <num> billion more than it anticipates from <num> through <num> because rising costs are counteracting gains in riders and revenue .\n",
            "Preliminary plans by Amtrak that were released yesterday call for stops of its high-speed express service in Boston , Providence , R.I . ,\n",
            "The agreement settles the first of three discrimination suits filed against Amtrak in the past two years .\n",
            "The Arizona Republican , chairman of the Senate Commerce Committee , led a highly critical oversight hearing Tuesday that painted a sobering portrait of Amtrak 's financial status and outlook .\n",
            "But he said Amtrak would make ends meet with new revenue , most of which is derived from one-time deals .\n",
            "The lawsuit is the second discrimination charge against Amtrak this year in US District Court in Washington .\n",
            "He said Amtrak 's governing board met last week and addressed much of the criticism in Mead 's audit .\n",
            "Earlier this year , Amtrak announced a high-speed service to lure travelers who now fly the lucrative Washington - New York - Boston shuttle route .\n",
            "In adding freight cars now to its regular passenger runs , he said , Amtrak will not allow any compromise in speed or reliability .\n",
            "Amtrak plans to name the trains Acela -LRB- pronounced uh-SELL-uh -RRB- , which is a made-up name , not an acronym .\n",
            "Amtrak is offering a deal it hopes few travelers can resist : Get good service or a free ride .\n",
            "The promise , McCain said , was that Amtrak would be self-sufficient after two years of federal operating subsidies .\n",
            "Amtrak said it would know in <num> days when the trains would be ready .\n",
            "Amtrak also is developing other high-speed rail corridors elsewhere , including one linking Illinois , Indiana , Iowa , Michigan , Minnesota , Missouri , Nebraska , Ohio , and Wisconsin .\n",
            "Amtrak spokesman John Wolf estimated that only one out of every <num> passengers will request a refund voucher .\n",
            "The plaintiffs ' lawyers said they want Amtrak to change its employment and disciplinary practices , and they are seeking damages for back pay and mental anguish .\n",
            "David Neeleman , the company 's chief executive , said recently that the airline will operate as Jetblue Airways .\n",
            "`` Amtrak is committed to treating all employees fairly , '' Amtrak President George Warrington said in a statement .\n",
            "WASHINGTON _ Amtrak managers filed a class-action lawsuit Thursday alleging that the company discriminated against African-Americans in management and other white-collar positions .\n",
            "Amtrak plans to put a customer-satisfaction guarantee into effect by the end of the year .\n",
            "Trains that do n't pass muster wo n't be permitted to leave , said John Wolf , an Amtrak spokesman .\n",
            "Amtrak Chairman Tommy Thompson , Wisconsin 's governor , told the committee the railway will meet the deadline .\n",
            "Officials of Amtrak refused to say what they would charge for the high-speed rail trip .\n",
            "Amtrak still faces hurdles , such as limited routes and schedules , in becoming a true alternative to airlines .\n",
            "Amtrak has agreed to settle a race discrimination lawsuit filed by black management employees and applicants for management positions .\n",
            "Congress gave Amtrak in <num> an infusion of aid along with a <num> deadline to become self-sufficient .\n",
            "'' Amtrak needs to make more progress before any further funding schemes are enacted , '' he said .\n",
            "Amtrak said it had the best on-time performance in <num> years .\n",
            "In response to the allegations , Amtrak officials said Thursday the company works to prevent discrimination .\n",
            "Amtrak said its new hiring would focus on people with traits that met customer-service needs .\n",
            "On the Net : Amtrak : http : //www . amtrak . com\n",
            "Congress created Amtrak in <num> from a collection of failing passenger railroads .\n",
            "Amtrak President George Warrington said settling out-of-court was the right thing to do .\n",
            "In July <num> , Amtrak settled a suit filed by black managers and employees seeking management positions .\n",
            "But in recent years we have made remarkable progress in turning Amtrak around. ''\n",
            "Amtrak 's ticket pricing for high-speed trains could be crucial to its success .\n",
            "Amtrak could face liquidation if it fails to meet the deadline .\n",
            "Analysts said they thought Amtrak could find a substantial niche in carrying high-value freight .\n",
            "It also alleged Amtrak maintained a racially hostile work environment .\n",
            "Amtrak : Good Service or Free Travel\n",
            "Amtrak ridership statistics go back to <num> .\n",
            "Amtrak Is Rolling Out Satisfaction Express\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU Check and Model Save"
      ],
      "metadata": {
        "id": "xFDqOBIt-ySt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "id": "h9D5DpGqjb15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "698ce02e-dbd2-445a-d5ca-eba3972a6b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.copyfile(\"/content/weights-Model--dc752ee900001d1-bestval.h5py\", \"/content/gdrive/MyDrive/FYPModels/weights-Model--dc752ee900001d1-bestval.h5py\")\n",
        "shutil.copyfile(\"/content/weights-Model--5c8bdb18d3a5047f-bestval.h5py\", \"/content/gdrive/MyDrive/FYPModels/weights-Model--5c8bdb18d3a5047f-bestval.h5py\")\n",
        "shutil.copyfile(\"/content/weights-Model--5c8bdb18d3a5047f-final.h5py.data-00000-of-00001\", \"/content/gdrive/MyDrive/FYPModels/weights-Model--5c8bdb18d3a5047f-final.h5py.data-00000-of-00001\")\n",
        "shutil.copyfile(\"/content/weights-Model--5c8bdb18d3a5047f-final.h5py.index\", \"/content/gdrive/MyDrive/FYPModels/weights-Model--5c8bdb18d3a5047f-final.h5py.index\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-APnRh5O1FVu",
        "outputId": "80347c6d-de05-4c11-a2f6-a810c8162b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/FYPModels/weights-Model--5c8bdb18d3a5047f-final.h5py.index'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "9ddfba760c93d0781cb88c4db9a31eb68ca4e1616821530f19d735f356a5c9ec"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "FYP_All_Modules_Combined_Fast_InOut.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YYYj9cnxZaT3",
        "zFEtkU9IVCbz",
        "1Ayb-Ueoa1z2",
        "XqbnSYTl2G5S",
        "RuCXJ_NWeAIf",
        "ISz5of2Eb8ip",
        "Yvi-JPsF-hv6",
        "KGV_IV5Tb8iq",
        "lJtVVxVieXZo",
        "iuhBwtrnncaX",
        "zrgMEX9ob8iz",
        "cusfEfH3b8i1",
        "xFDqOBIt-ySt"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}